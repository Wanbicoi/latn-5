UNIVERSITY OF SCIENCE, VNU-HCM
FACULTY OF INFORMATION TECHNOLOGY
ADVANCED PROGRAM IN COMPUTER SCIENCE
NGUYỄN QUANG THỨC - NGUYỄN NGỌC MINH HUY
INTELLIGENT ANNOTATION ASSISTANCE
WITH REFERRING EXPRESSION AND
VIDEO OBJECT SEGMENTATION
BACHELOR OF SCIENCE IN COMPUTER SCIENCE
HO CHI MINH CITY, 2021
UNIVERSITY OF SCIENCE, VNU-HCM FACULTY OF INFORMATION TECHNOLOGY
ADVANCED PROGRAM IN COMPUTER SCIENCE
NGUYỄN QUANG THỨC - NGUYỄN NGỌC MINH HUY
INTELLIGENT ANNOTATION ASSISTANCE WITH REFERRING EXPRESSION AND
VIDEO OBJECT SEGMENTATION
BACHELOR OF SCIENCE IN COMPUTER SCIENCE
HO CHI MINH CITY, 2021
UNIVERSITY OF SCIENCE, VNU-HCM
FACULTY OF INFORMATION TECHNOLOGY
ADVANCED PROGRAM IN COMPUTER SCIENCE
NGUYỄN QUANG THỨC 1751037
NGUYỄN NGỌC MINH HUY 1751030
INTELLIGENT ANNOTATION ASSISTANCE
WITH REFERRING EXPRESSION AND
VIDEO OBJECT SEGMENTATION
BACHELOR OF SCIENCE IN COMPUTER SCIENCE
THESIS ADVISOR
ASSOC. PROF. TRẦN MINH TRIẾT
HO CHI MINH CITY, 2021
PCS IN
TELLIG EN
T A N
N O
TA TIO
N A
SSISTA N
CE W ITH
REFERRIN G
EX PRESSIO
N A
N D
V ID
EO O
BJECT SEG M
EN TA
TIO N
2021
UNIVERSITY OF SCIENCE
ADVANCED PROGRAM IN COMPUTER SCIENCE
COMMENTS OF THESIS’S ADVISOR
(Research)
Thesis title:
INTELLIGENT ANNOTATION ASSISTANCE WITH REFERRING EXPRESSION
AND VIDEO OBJECT SEGMENTATION
Students: Nguyễn Quang Thức (1751037) – Nguyễn Ngọc Minh Huy (1751030)
Advisor: Assoc. Prof. Trần Minh Triết
1. Research Topics and Ideas:
The objectives of the thesis are:
- Support annotators to label visual data based on the detailed description of the object. The
students proposed a hybrid approach of an end-to-end model and a rule-based model. With
the rule-based method, the students also proposed an add-on architecture to support multiple
attributes in the future.
- Develop a flexible web application platform that supports additional integrating algorithms
as modules which also is responsive and works on both desktop and mobile devices.
2. Research Methods:
The students learned the technologies and frameworks to build a complete web application.
They also apply design patterns and architecture design to create a flexible platform that can
be scaled up and extended with more features and functionalities.
The student group explored the different deployment strategies for serving the website in the
production environment. They have delivered the system which consists of three main
components: frontend, backend, and model servers. The platform allows users to annotate
image and video data with interactive support from deep learning models to speed up the
annotation process.
They conduct literature reviews about existing methods for the referring expression tasks.
Then, based on their observations and experiments, they propose modifications to the existing
Cross-Modal Progressive Comprehension (CMPC) model and achieve improvements in
accuracy while reducing the model size and training times compared to the original model.
Finally, they build a rule-based algorithm as an augmentation for referring expressions to solve
some specific object attributions: object category, color, and human gender.
3. Contributions:
In this thesis, the students built an annotation platform for image and video on the web, which
supports users to select and segment objects in image and video. The main characteristic of their
platform is supporting referring expression segmentation, where users can interact with the
platform by using natural language to describe the appearance. In terms of the algorithm, they
improved the performance of prior work and evaluated the algorithm on Youtube-RefVOS.
They achieved a 0.473 overall J&F score, standing on the top 6 of the leaderboard of validation
sets. Moreover, they also proposed an algorithm to match attributes of the objects in the image
with attributes of the query sentence to select the best candidate. Currently, they support three
attributes: object class, color, and gender.
4. Report:
The thesis report has 8 chapters with 112 pages. The report has logical structure and is well
written.
5. Presentation:
The students carefully prepared for the presentation.
6. Publications and/or real-world applications:
Their platform successfully deployed on two domains:
- http://smartannotation.selab.hcmus.edu.vn/
- https://smart-annotation-platform.live/
This is a module in Intelligent Annotation Platform of the project VinIF2019.DA19:
- To create datasets for Intelligent Traffic System (ITS) with data in Vietnam
- To create datasets for video object segmentation
- To create a repository of segmented object sequences for generating/synthesizing
scene/video from description
Rank: (please choose one of the following
categories:
Fair/Good/Excellent/Outstanding)
OUTSTANDING
Ho Chi Minh city, August 29, 2021
Advisor
Trần Minh Triết
UNIVERSITY OF SCIENCE
ADVANCED PROGRAM IN COMPUTER SCIENCE
COMMENTS OF THESIS’S REVIEWER
(Research)
Thesis title:
INTELLIGENT ANNOTATION ASSISTANCE WITH REFERRING EXPRESSION AND
VIDEO OBJECT SEGMENTATION
Students: Nguyễn Quang Thức (1751037) – Nguyễn Ngọc Minh Huy (1751030)
Advisor: Assoc. Prof. Dr. Trần Minh Triết
1. Research Topics and Ideas:
The students developed a smart annotation platform on the web environment for computer
vision tasks: annotating images and videos by bounding boxes, polygons, and masks. They also
integrated labeling automation using deep learning models to aid the annotation process for
saving annotators’s work and time. One of the interesting features of the system is that the
students propose an enhancement of Referring Expression approach for Video Object
Segmentation. This is a natural means of interacting to assist efficiently to annotators by
describing which object/person to be segmented.
2. Research Methods:
The students integrate two methods: Cross-Modal Progressive Comprehension (CMPC) and
Rule-based algorithm. They inherit the CMPC model then modify it based on their experiments.
Moreover, they also proposed a strategy for applying this method to video data type by reuse
modules from ‘Modular Interactive Video Object Segmentation: Interaction-to-Mask,
Propagation and Difference-Aware Fusion (MiVOS)’. Furthermore, with the rule-based
method, they extract and compare the attributes extracted from the query and attributes
extracted from the object.
3. Contributions:
- The students deployed a smart annotation platform on the domain:
http://smartannotation.selab.hcmus.edu.vn which is responsive and available from both desktop
and mobile devices for labeling computer vision dataset with intelligent assistance for
annotators using deep learning models.
- The students introduce new means of annotation by using natural language referring
expressions. They evaluate the methods on dataset Youtube-RefVOS 2021 and reach 0.473
overall J&F scores and the top 6 on the world-wide leaderboard.
- The student also develop a rule-based algorithm to handle some specific cases and easily scale
up a number of supported attributes based on the demand of the annotator.
4. Report:
The report is represented in a well structure including software architecture and deep learning
models for the interactive annotation system.
5. Presentation:
The slide is carefully prepared, very nice in both outlook and content.
6. Publications and/or realworld applications:
The student successfully deployed the system to the university website
http://smartannotation.selab.hcmus.edu.vn, and the interaction is very easy to use. This tool is
being used to create datasets for intelligent traffic systems with data in Vietnam, and to create
a repository of segmented video object sequences for synthesizing new scene for training as an
efficient data augmentation.
Rank: Outstanding (10)
Ho Chi Minh city, 28/8/2021
Reviewer
Nguyễn Vinh Tiệp
Acknowledgements
Firstly, we would like to express our special thanks of gratitude to our advisor,
Assoc. Prof. Trần Minh Triết for his valuable guidance, keen interest, and encour-
agement during our thesis work. He helps us see many new perspectives on our
problems and profound lessons around them. They are valuable experiences for us
to continue our future study and research path.
Secondly, we also wish to extend our appreciation to our lecturers in the Advance
Program in Computer Science and the Faculty of Information Technology. They
are enthusiastic about teaching and guiding students. We gained lots of knowledge
from them during our four years in university, which formed a solid foundation for
accomplishing this project.
Thirdly, we want to thank our friends for their support of experience, knowledge,
and idea.
Finally, we express our grateful attitude to our families, who are our spiritual
support and are always beside us to care for and raise us.
v
UNIVERSITY OF SCIENCE
ADVANCED PROGRAM IN COMPUTER SCIENCE
Thesis Proposal
Thesis title:
INTELLIGENT ANNOTATION ASSISTANCE WITH REFERRING EXPRESSION
AND VIDEO OBJECT SEGMENTATION
Thesis advisor: Assoc. Prof. Trần Minh Triết
Students: Nguyễn Quang Thức (1751037) – Nguyễn Ngọc Minh Huy (1751030)
Type of thesis: Research with demo application
Duration: January 01, 2021 to August 30, 2021
Contents of thesis:
1. Introduction
Data annotation is an important problem in the machine learning industry because
models need vast data to reach high performance. As a result, many platforms have been
developed to help annotators work efficiently and accurately, such as supervise.ly,
labelbox.com, VoTT, superannotate.com, CVAT, etc.
The annotation platform involves an annotation tool and data management. It also
provides some algorithms which support labeling tasks. Based on the supported
algorithm, we divide the annotation platforms into two groups: non-AI-assisted
platforms, which use some classic computer vision algorithms, and AI-assisted
platforms, which use deep learning to provide suggestions.
A standard annotation tool consists of interacting and drawing functions. Interacting
tools provide ways to change views of the image, such as zoom in/out, dragging the
viewport, etc., and image processing tools to enhance image quality, such as adjusting
the image's contrast and brightness, etc. Drawing tools are specifically due to the
dataset's requirements (bounding box, segmentation mask, landmarks). Generally,
drawing a polygon by clicking and dragging the mouse is familiar with predefined
shapes: rectangle, circle, etc. Additional annotation types are point, line strings. In non-
AI-assisted platforms, classical computer vision algorithms such as superpixel, contour
segmentation are applied as aids for the annotation process.
The AI-assisted platform supports some particular workflows for the project (human-
in-the-loop, labeling consensus, ML-based quality-check). Additionally, these
platforms' core provides some general AI-assisted tools, which boosts the annotator's
performance. They provide auto-generated suggestions during the annotation process
and introduce new interactions. Two main suggestion types depend on the way the
model gives the prediction. Notably, in the first approach, the model will predict and
give estimations for positions and shapes of objects based on predefined classes without
using humans’ interaction. Then, humans refine the predictions of the model. In the
second approach, the annotator needs to guide the model by using simple interactions.
In detail, instead of requiring precise annotations, annotators are only required to give
weak annotations as input for models to produce the segmentation masks.
Many works show that using weak annotation reduces lots of effort during labeling. For
example, the COCO annotator [4] takes advantage of Deep Extreme Cut (DEXTR) [5].
It only requires the annotator to make only four clicks (top, bottom, left, and right most
points) for each object to produce a segmentation mask. PathTrack [6] introduces simple
interaction for annotating tracks of object masks in videos by using the cursor to follow
the object in the video's duration before the model generates the annotation for the
whole track itself. It is shown that the annotation process is only 33% slower than
watching the video normally. Platforms that support annotation for object tracking in
videos (BubbleNets, LabelBox, Playment [7, 8, 9]) use keyframes suggestion and
interpolation to let users annotate only a small fraction of frames while keeping a high
accuracy of the track.
However, both approaches still have some problems. Without human interaction, the
first one can limit the model by learned knowledge belonging to the training set. It thus
provides poor predictions on unseen classes. Some platforms address the problem by
using human-in-the-loop (HITL) and active learning workflow. Concretely, each
iteration in HITL consists of three main steps: (1) users label a small batch of the dataset,
(2) the model uses annotated data for training, (3) apply the trained model for unlabeled
data. Thus, since the second iteration, the model can be used for suggesting. In addition,
some platforms integrate active learning to sort the order of data during the annotation
process to improve performance gain after each iteration. The cons of this approach are
that it requires a lot of time for training with the new concepts, and it still requires
manual annotation in the first iteration. In the second approach, some of them do not
clearly define which object needs annotation. Thus, it leads to inaccurate suggestions
and consumes lots of time for refinement.
Although the existing platform has provided multiple gestures and features to generate
annotations, there are limitations in defining target objects' categories. Categories are
defined using general categories to represent the whole class (e.g., “cat”, “dog”, “car”)
which do not include visual information about the object and can not be recognized
without advanced knowledge. Referring expression tasks aim to align computer vision
and natural language components by locating the referenced object based on a given
expression [10, 11, 12, 13, 14, 15, 16] with detailed information about the category,
size, color, relative position, … Besides, referring expressions can give a more detailed
description of the object that can be expressed for indicating accurate object instances
and distinguishing them from other objects in the same category. Referring expression
introduces natural language utterances for human-computer interaction and has
received much attention in other fields such as intelligent user interfaces.
2. Motivation
Deep learning is growing significantly and has various applications in stock prediction,
recommendation systems, ultrasonography, digital pathology, defect detection, and
preventative maintenance. However, the performance of a deep learning model is highly
dependent on the quality of the dataset. Thus, many organizations release their public
datasets for the science community, but it still does not cover all the needed problems.
One of the most popular fields in deep learning is computer vision, which has many
different data formats, such as video, single image, multidimensional data from
scanners (e.g. 3D scanner, medical scanner). The development of digital data is very
fast, which also leads to a significant increase in the number of image data. However,
most of them are raw data that rarely can be used directly due to inconsistent quality,
information, and distribution of each type. Labeling these data takes a lot of effort,
although some of the steps can be done automatically. For example, with 350,000 hours
of video in the Youtube-8M dataset [2] containing 30,240,000 frames, humans still need
to manually annotate each frame even if the difference between two consecutive frames
is small.
In the real industry, the cost of data labeling for any product is costly. In different
phases, the number of annotators can be varied. For example, it requires about 100
annotators when products have a small scale, but it can raise to 1000 annotators for
large-scale production [1]. Thus, it consumes up to $20M annually if the cost for a
human annotator is $7 per hour [1]. So, there are many annotation platforms assisted by
AI that are built to reduce time consumption.
Existing platforms, which predict without human interaction, focus on the performance
of models with predefined class sets, are not flexible enough with the complex
requirements. For example, some dataset needs classify the object based on the
attributes and the fine-grained description in some situations instead of the object
category. It thus requires the model to understand the context.
These problems encourage us to adopt referring expressions for visual objects and some
algorithms to support video labeling. Our platform aims to locate the object more
accurately based on the detailed description (e.g., color, shape, etc.) with referring
expressions and provide some strategies to reduce the cost for video labeling by using
mask propagation.
3. Objectives
Our goal is to integrate referring expressions and mask propagation into the intelligent
annotation platform. The first objective is supporting annotator labeling based on the
detailed description of the object. Then, we aim to reduce the cost of video labeling and
develop a flexible platform, which supports the integration of additional algorithms as
modules.
To obtain the objectives, we point out the main tasks relating to:
a. Web app development:
- UI/UX design
- Implementing tools for image interaction and annotation
- Database and architecture design
- Storage management for large amounts of images and videos
- Design and deployment of Machine Learning system architecture
- Testing and examining user experience
b. Annotation tool features:
- Literature review about existing annotation tools
- Examining good features for implementation
- Finding library support for corresponding features
- Testing the performance and effectiveness of each feature
c. Intelligent suggestions for annotations:
- Literature review about referring expression
- Selecting models for implementation
- Training and fine-tuning the models
Research timelines:
01/01/2021: Literature review about annotation platforms
15/01/2021: Literature review about referring expression segmentation.
01/02/2021: Reproduced some existing referring expression methods.
15/02/2021: Study the architecture of Machine Learning systems.
01/03/2021: Study the deployment process of Machine Learning systems.
15/03/2021: Study design patterns.
01/04/2021: Design the architecture of the system
01/05/2021: Implement the annotation platform and improve RES model
01/07/2021: Writing the thesis
References
[1] Ajinkya Malasane “Decision Framework For Data Labeling Strategy.” Best-in-
Class Data Labeling Platform | Playment. https://playment.io/blog/refine-your-data-
labeling-strategy-with-a-realistic-decision-framework.
[2] Lee, J., (Paul) Natsev, A., Reade, W., Sukthankar, R., & Toderici, G. (2018). The
2nd YouTube-8M Large-Scale Video Understanding Challenge. In Proceedings of the
European Conference on Computer Vision (ECCV) Workshops.
[3] Joyce Y Chai, Pengyu Hong, and Michelle X Zhou. 2004. A probabilistic approach
to reference resolution in multimodal user interfaces. In Proceedings of the 9th
international conference on Intelligent user interfaces. ACM, pages 70–77.
[4] Brooks, J. (2019). COCO Annotator. Retrieved from
https://github.com/jsbroks/coco-annotator/
[5] Kevis-Kokitsi Maninis, Sergi Caelles, Jordi Pont-Tuset, & Luc Van Gool. (2018).
Deep Extreme Cut: From Extreme Points to Object Segmentation.
[6] Santiago Manen, Michael Gygli, Dengxin Dai, & Luc Van Gool. (2017). PathTrack:
Fast Trajectory Annotation with Path Supervision.
[7] Brent A. Griffin, & Jason J. Corso. (2020). BubbleNets: Learning to Select the
Guidance Frame in Video Object Segmentation by Deep Sorting Frames.
[8] Labelbox, Inc. Labelbox. Retrieved from https://labelbox.com/.
[9] Playment, Inc (2021). Playment. Retrieved from https://playment.io/.
[10] Qiao, Y., Deng, C., & Wu, Q. (2020). Referring Expression Comprehension: A
Survey of Methods and Datasets. ArXiv, 1–15.
[11] Yu, L., Lin, Z., Shen, X., Yang, J., Lu, X., Bansal, M., & Berg, T. L. (2018).
MAttNet: Modular Attention Network for Referring Expression Comprehension.
Proceedings of the IEEE Computer Society Conference on Computer Vision and
Pattern Recognition, 1307–1315. https://doi.org/10.1109/CVPR.2018.00142
[12] Xihui Liu, Zihao Wang, Jing Shao, Xiaogang Wang, & Hongsheng Li. (2019).
Improving Referring Expression Grounding with Cross-modal Attention-guided
Erasing.
[13] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, & Jifeng Dai.
(2020). VL-BERT: Pre-training of Generic Visual-Linguistic Representations.
[14] Luo, G., Zhou, Y., Sun, X., Cao, L., Wu, C., Deng, C., & Ji, R. (2020). Multi-task
collaborative network for joint referring expression comprehension and segmentation.
Proceedings of the IEEE Computer Society Conference on Computer Vision and
Pattern Recognition, 10031–10040. https://doi.org/10.1109/CVPR42600.2020.01005
[15] Ronghang Hu, Marcus Rohrbach, & Trevor Darrell. (2016). Segmentation from
Natural Language Expressions.
[16] Ruiyu Li, Kaican Li, Yichun Kuo, Michelle Shu, Xiaojuan Qi, Xiaoyong Shen, and
Jiaya Jia. Referring image segmentation via recurrent refinement networks. In CVPR,
2018.
Approved by the advisor Ho Chi Minh city, 03/01/2021
Signature of advisor Signature(s) of student(s)
Trần Minh Triết Nguyễn Ngọc Minh Huy Nguyễn Quang Thức
Contents
Acknowledgements v
Contents xiii
Abstract xxi
1 Introduction 1
1.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.1.1 Data annotation . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.3 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.4 Thesis outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2 Related work 8
2.1 Referring Expression Comprehension
and Segmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2.1.1 Task overview . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2.1.2 Methods overview . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.2 Multi-modal fusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.3 Referring Image Segmentation via Cross-Modal Progressive Com-
prehension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2.4 Annotation platform . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.4.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.4.2 Annotation platform features . . . . . . . . . . . . . . . . . . 18
2.4.3 Data labeling automation . . . . . . . . . . . . . . . . . . . . 19
2.4.4 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
xiii
3 Creating a web application using ReactJS 24
3.1 React Library . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
3.1.1 Overview of React library . . . . . . . . . . . . . . . . . . . . 24
3.1.2 React features . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
3.1.3 React web environment . . . . . . . . . . . . . . . . . . . . . 25
3.1.4 Extendability using JavaScript libraries . . . . . . . . . . . . 25
3.2 React Component . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
3.2.1 Function and Class Components . . . . . . . . . . . . . . . . 26
3.2.2 Component lifecycles . . . . . . . . . . . . . . . . . . . . . . . 27
3.2.3 Component props . . . . . . . . . . . . . . . . . . . . . . . . . 31
3.2.4 Component state . . . . . . . . . . . . . . . . . . . . . . . . . 32
3.2.5 Handling events . . . . . . . . . . . . . . . . . . . . . . . . . . 33
3.3 React data flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
3.3.1 Top-down data flow . . . . . . . . . . . . . . . . . . . . . . . . 35
3.4 React Hooks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
3.4.1 Hooks overview . . . . . . . . . . . . . . . . . . . . . . . . . . 36
3.4.2 Using the State hook . . . . . . . . . . . . . . . . . . . . . . . 37
3.4.3 Using the Effect hook . . . . . . . . . . . . . . . . . . . . . . 38
3.4.4 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
4 Frontend Architecture 41
4.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
4.2 Modular structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
4.2.1 Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
4.2.2 Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
4.3 State management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
4.3.1 Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
4.3.2 Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
4.4 Event based communication between modules . . . . . . . . . . . . . 49
4.4.1 Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
4.4.2 Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
5 Backend Architecture 53
5.1 Flask framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
xiv
5.1.1 Overview of Flask framework . . . . . . . . . . . . . . . . . . 53
5.1.2 Flask views and routes . . . . . . . . . . . . . . . . . . . . . . 54
5.1.3 Flask Blueprint . . . . . . . . . . . . . . . . . . . . . . . . . . 55
5.1.4 Request parsing . . . . . . . . . . . . . . . . . . . . . . . . . . 57
5.2 Cloud storage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
5.2.1 Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
5.2.2 Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
5.2.3 Browser caching . . . . . . . . . . . . . . . . . . . . . . . . . . 59
5.3 Deployment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
5.3.1 Docker . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
5.3.2 Google Cloud Run . . . . . . . . . . . . . . . . . . . . . . . . 63
5.4 Model serving . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
5.4.1 Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
5.4.2 Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
6 Smart Assistance for Instance Segmentation with Referring Ex-
pression 69
6.1 Cross-modal Progressive Comprehension . . . . . . . . . . . . . . . . 69
6.1.1 Entity Perception . . . . . . . . . . . . . . . . . . . . . . . . . 70
6.1.2 Relation-aware Reasoning . . . . . . . . . . . . . . . . . . . . 72
6.1.3 Text-Guide Features Exchanged . . . . . . . . . . . . . . . . 73
6.2 Decoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
6.3 Mask fusion module . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
6.4 Rule-based algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
6.4.1 Textual Attribute Extraction . . . . . . . . . . . . . . . . . . 77
6.4.2 Visual Attribute Extraction . . . . . . . . . . . . . . . . . . . 78
6.4.3 Matching module . . . . . . . . . . . . . . . . . . . . . . . . . 78
7 Smart Annotation Platform 80
7.1 Overall architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
7.2 Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
7.2.1 Data management . . . . . . . . . . . . . . . . . . . . . . . . 81
7.2.2 Labeling tools . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
7.3 Annotation with intelligent assistance flow . . . . . . . . . . . . . . . 93
xv
7.4 Propagation flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
8 Conclusion 99
8.1 Platform Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
8.2 Algorithm evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
8.2.1 Implementation Detail . . . . . . . . . . . . . . . . . . . . . . 100
8.2.2 Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . 101
8.2.3 Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . 101
8.3 Experiment results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
8.3.1 CMPC model . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
8.4 Future work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
References 110
xvi
List of Figures
1.1 Percentage of Time Allocated to Machine Learning Project Tasks . 2
2.1 Sample in G-Ref dataset . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.2 a) Single-frame query b) Full-video query . . . . . . . . . . . . . . . 10
2.3 Source: Neighborhood Watch: Referring Expression Comprehension
via Language-guided Graph Attention Networks . . . . . . . . . . . 11
2.4 Source: Learning to Assemble Neural Module Tree Networks for
Visual Grounding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.5 Source: Graph-Structured Referring Expression Reasoning in The
Wild . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.6 Source: A Fast and Accurate One-Stage Approach to Visual Ground-
ing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.7 Source: Encoder Fusion Network with Co-Attention Embedding for
Referring Image Segmentation . . . . . . . . . . . . . . . . . . . . . . 14
2.8 Source: Decoupled Spatial-Temporal Graphs . . . . . . . . . . . . . 14
2.9 Source: Unified Referring Video Object Segmentation Network . . . 15
2.10 Source: Referring Image Segmentation via Cross-Modal Progressive
Comprehension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2.11 Key elements of Data Annotation Tools . . . . . . . . . . . . . . . . 17
2.12 Data management screen shot from SuperAnnotate.com . . . . . . . 19
2.13 Labeling for video tool from supervise.ly . . . . . . . . . . . . . . . . 20
2.14 Weak annotation by 4 clicks using Deep Extreme Cut . . . . . . . . 21
2.15 Pre-labeling tool of playment.io allow users to set the confidence
score threshold for filtering detected object bounding boxes . . . . . 22
2.16 Active Learning basic flow . . . . . . . . . . . . . . . . . . . . . . . . 23
3.1 "Hello World" h1 element rendered by code snippet in Listing 3.3 . 27
xvii
3.2 Class component common lifecycle . . . . . . . . . . . . . . . . . . . 27
3.3 Top-down data flow and lifting state up combined into full React
data flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
4.1 The application is divided into multiple modules and attached to
the core. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
4.2 The module root selects the appropriate module to run depending
on the current state of the core (in this case “mode”) . . . . . . . . . 44
4.3 Replacing module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
4.4 Flux architecture workflow . . . . . . . . . . . . . . . . . . . . . . . . 46
4.5 Zustand architecture workflow . . . . . . . . . . . . . . . . . . . . . . 47
4.6 Horizontal communication problem between modules . . . . . . . . 50
4.7 Event flow in our application . . . . . . . . . . . . . . . . . . . . . . 51
5.1 Most popular python web frameworks in Python Developers Survey
2020 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
5.2 Example of endpoints in a annotation platform API, where each
color indicating grouped functions forming an individual Blueprint
and injected into the Flask app . . . . . . . . . . . . . . . . . . . . . 56
5.3 Upload and get file from Google Cloud Storage flow . . . . . . . . . 60
5.4 Caching by setting cache control max-age . . . . . . . . . . . . . . . 61
5.5 Validating the resource version before using in no-cache strategy . . 62
5.6 Model-as-service prediction request and response flow . . . . . . . . 66
6.1 CMPC Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
6.2 CMPC Stage 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
6.3 Text-guided Feature Exchanged at one round for level 4 . . . . . . . 74
6.4 Pipeline after adopt decoder of DeepLabV3+ . . . . . . . . . . . . . 75
6.5 Rule-based pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
7.1 Overall architecture of our annotation platform . . . . . . . . . . . . 81
7.2 Layout of project management page . . . . . . . . . . . . . . . . . . 82
7.3 Layout of dataset management page of one project . . . . . . . . . . 83
7.4 Layout of dataset info page which showing dataset detail and data
instances belong to that dataset . . . . . . . . . . . . . . . . . . . . . 84
xviii
7.5 Layout of label management in a project. Creating a new label will
open a pop up for user to fill in name and choose colors. . . . . . . 85
7.6 Layout of video annotation page . . . . . . . . . . . . . . . . . . . . 86
7.7 Layout of annotation object list . . . . . . . . . . . . . . . . . . . . . 87
7.8 Layout of data instance list panel . . . . . . . . . . . . . . . . . . . . 88
7.9 Layout of label list panel . . . . . . . . . . . . . . . . . . . . . . . . . 89
7.10 Layout of video controller . . . . . . . . . . . . . . . . . . . . . . . . 89
7.11 Layout of frame list and propagation control . . . . . . . . . . . . . 90
7.12 Bounding box annotations . . . . . . . . . . . . . . . . . . . . . . . . 91
7.13 Polygon annotations . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
7.14 Scribbles to mask . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
7.15 Referring expression to mask . . . . . . . . . . . . . . . . . . . . . . . 94
7.16 Annotation with intelligent assistance flow . . . . . . . . . . . . . . . 95
7.17 Preview-and-Annotation diagram . . . . . . . . . . . . . . . . . . . . 97
8.1 Predicted mask at each level for calculating multi-level loss. Mask
from left to right corresponds to mask at level res3, res4, res5, and
final mask . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
8.2 The models run well on query with keyword "left" and "right" in
case of having less than three objects in the image. . . . . . . . . . . 103
8.3 Fail case with many objects in the scene . . . . . . . . . . . . . . . . 104
8.4 Model performance in cases of query described by relative position
to other objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
8.5 Model performance in querying different animal kinds . . . . . . . . 105
8.6 Model does not work well in case of complex scene with many ani-
mals while it can understand wrong queries. . . . . . . . . . . . . . . 106
8.7 The model performs pretty well for expressions with colors . . . . . 107
8.8 The model has the sense about size of objects. However the masks
are not very good in this kind of query. . . . . . . . . . . . . . . . . 107
8.9 Results of the model with query about genders and some kinds of
in-house objects. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
xix
List of Tables
8.1 Ablation study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
xx
Abstract
Artificial Intelligence is becoming a popular field in recent years, especially in
natural language processing and computer vision. However, it also raises a problem
about data shortage, where most of the deep learning model requires a vast amount
of data for training. Thereby, annotation platforms were born to help speeding up
the labeling process.
Annotation platforms can be divided into two types: AI-assisted and non-AI-
assisted. With AI-assisted, the platform will automate some tasks such as auto-
predict, tracking, segmentation with human interaction, etc. In contrast, the non-
AI-assisted platform will focus on managing projects and simple tools for anno-
tation. Although many types of interaction were available such as click, draw, it
still does not use one of the most natural ways for human-computer interaction,
natural language utterances.
In this thesis, we design an annotation platform that supports referring expres-
sion for image labeling. Moreover, in video labeling, we realized that manually
labeling each frame was unnecessary and wasteful because the difference between
consecutive frames was so slight. The current state-of-the-art models can easily
handle tracking tasks and propagation tasks based on some manual annotations.
This thesis proposed enhancements to reduce the model’s weight and improve
the mask’s boundary of Cross-modal Progressive Comprehension. In particular,
we eliminate the contribution of low-level features and adopt a decoder branch
DeeplabV3+ to refine the segmentation boundary. Finally, we suggest an algorithm
to mitigate the inconsistency when predicting on video. We evaluate our method on
Youtube-Refvos and reach 0.473 overall J&F score. Moreover, we also suggest an
architecture for the rule-based method, which supports queries with many attribute
types.
We developed an annotation platform on web environment that can be accessed
xxi
from both computer or mobile devices. Our platform allows users to create new
datasets by annotating images and videos by bounding boxes, polygons, and masks.
We also integrated labeling automation using deep learning models to aid the
annotation process by saving annotators’ work and time. Intelligent assistance
provides supports for converting scribbles to mask, mask propagation through video
frames, referring expression to mask including our enhanced version of Cross-modal
Progressive Comprehension model and rule-based one.
xxii
Chapter 1
Introduction
In Chapter 1, we introduce the data annotation industry and how it plays
an important role in the Artificial Intelligent field. Then, we present our
motivation in adopting Referring Expression into the annotation platform
to enhance the way that people interact with the platform. We then present
the motivation and objective of our work. Finally, we describe the outline
content of this report.
1.1 Overview
1.1.1 Data annotation
Data annotation is the process of identifying raw data (images, text, videos,
etc.) and adding one or more meaningful and informative labels so that machine
learning models can learn from it. For example, in object segmentation for images,
the annotation task might involve defining a mask to indicate which region of
the image contains a person or a car. Data annotation is required for a variety
of use cases including computer vision, natural language processing, and speech
recognition.
As artificial intelligence and machine learning continue to grow, they come in
high demand for large annotated datasets. Data is crucial in training, testing,
validating, and supporting machine learning algorithms. The higher complexity
of the model, the more data is required for training the models to achieve high
accuracy. Model performance does not only depend on quantity but also the quality
1
of data. High-quality data labeling yields better model performance. In contrast,
when data labels are of low quality, machine learning models struggle to learn and
may lead to underperforming the potential capability of the model. Therefore, the
process of preparing data is an important part of any machine learning project
that takes up a considerable amount of time and human resources.
Figure 1.1: Percentage of Time Allocated to Machine Learning Project Tasks
Source: Data Engineering, Preparation, and Labeling for AI 2019
A recent report from Cognilytica [1] in 2019 finds that over 80% of the time
enterprises spend on AI projects goes toward collecting, preparing, cleaning, and
labeling data. As can be seen from Figure 1.1, the stage that consumes the highest
proportion of projects is data labeling. Most of the data available today is raw
data which is not ready to be used by machine learning applications and requires
significant effort in preparation. In supervised or semi-supervised tasks, the algo-
rithms must be trained on ground truth data that has been labeled with whatever
information the model needs and try to learn. For example, in image recognition
tasks, you must have an adequate amount of data that is already labeled with all
the classes the model is going to recognize. This stage of data labeling work is
usually very human-intensive.
Due to the demand for data annotation, many platforms have been developed
2
to help annotators work efficiently and accurately. Annotation platforms involve
an annotation tool and data management. They also provide some algorithms
from classical computer vision to applying deep learning models for supporting
annotators during the labeling process.
1.2 Motivation
Deep learning is growing significantly in many aspects and has various applica-
tions in stock prediction, recommendation systems, ultrasonography, digital pathol-
ogy, defect detection, and preventative maintenance. The performance of a deep
learning model is highly dependent on the quality of the dataset. It leads to many
organizations releasing their public datasets for the science community, but it still
does not cover all the needed problems.
One of the most popular fields in deep learning is computer vision, which has
many different formats of data, such as video, single image, multidimensional data
from scanners (e.g. 3D scanner, medical scanner). The development of digital data
is very fast, which also leads to a significant increase in the number of image data.
However, most of them are raw data that is rarely used directly due to inconsistent
quality, information, and distribution of each type. Labeling these data takes a
lot of effort although some of the steps can be done automatically. For example,
with 350,000 hours of video in Youtube-8M dataset [2] contains 30,240,000 frames,
humans still need to annotate each frame manually even if the difference between
two consecutive frames is small.
In the real industry, the cost of data labeling for any product is costly. In
different phases, the number of annotators can be varied. For example, it requires
about 100 annotators when products have a small scale, but it can raise to 1000
annotators for large-scale production [3]. It thus consumes up to $20M annually if
the cost for a human annotator is considered as $7 per hour [3]. So, there are a lot
of annotation platforms assisted by AI that are built to reduce time consumption.
Although the existing platforms have provided multiple gestures and features
to generate annotations including ones that using deep learning models for sug-
gestions, most of assisting annotation features are highly dependent on the per-
formance of models with fixed predefined class sets, are not flexible enough with
the difficulty of real scenarios requirements. For example, in some situations, the
3
dataset needs to classify the object based on the attributes and the fine-grained
description of the object instead of the kind of object. It thus requires the model
to understand the context.
In opposition, referring expression introduces natural language utterances for
human-computer interaction, so it has received much attention in other fields as
intelligent user interfaces [4]. Referring expression tasks aim to align computer
vision and natural language components by locating the referenced object based on
a given expression about the category, size, color, relative position, which support
to indicate object instances and distinguish target object from other objects in the
same category. Therefore, referring expression has more flexibility and capability in
describing objects comparing to existing methods in current annotation platforms.
However, referring expression is not widely in the annotation industry.
These problems encourage us to adopt referring expressions for visual objects
and some algorithms to support video labeling. Our platform aims to locate the
object more accurately based on the detailed description (e.g color, shape, etc.)
with referring expressions, and provide some strategies to reduce cost for video
labeling by using mask propagation.
1.3 Objectives
Our goal is to integrate referring expressions and mask propagation into the in-
telligent annotation platform. The first objective is supporting annotator labeling
based on the detailed description of the object. Therefore, we proposed a hybrid
approach of end-to-end model and rule-based model. With the rule-based method,
we also proposed an add-on architecture to support multiple attributes in the fu-
ture. Moreover, we also aim to develop a flexible platform that supports additional
integrating algorithms as modules.
The detailed work in this thesis includes
•
Web app development:
– UI/UX design
– Implementing tools for image interaction and annotation
– Database and architecture design
4
– Storage management for large amounts of images and videos
– Design and deployment of Machine Learning system architecture
– Testing and examining user experience
•
Annotation tool features
– Literature review about existing annotation tools
– Examining good features for implementation
– Finding library support for corresponding features
– Testing the performance and effectiveness of each feature
– Smart suggestions for annotations
•
Referring Expression:
– Literature review about referring expression
– Selecting models for implementation
– Training and fine-tuning the models
Our main contributions are as follows:
•
We inherit Cross-modal Progressive Comprehension and modify it based on
our experiment and hypothesis in the end-to-end model approach. Concretely,
we remove the contribution of low-level features based on our experiments,
where the mask prediction from that level is bad. The evaluation shows that
this modification degrades the score of object boundary, so we thus suggest
adopting the decoder branch of DeeplabV3+ to enhance the mask. To apply
this method to video type, we proposed a Mask-fusion strategy to improve the
mask consistency.
•
With the desire to support more attributes in the query sentence, we develop
an architecture that helps process each attribute independently as add-ons. In
particular, we first extract objects in the image by using instance segmentation.
Then, we create a triple module for each attribute, one for processing text and
one for processing the object, and one for calculating a matching score for that
attribute. Therefore, our system can easily scale up the number of attribution
and make the query sentence flexible and personalized with each person.
5
•
A smart annotation platform for labeling computer vision dataset with intel-
ligent assistance for annotators by the help of deep learning model. We also
introduce new means of annotation by using natural language referring ex-
pressions. The platform has a flexible architecture that allows new extensions
and adding functions as well as new auto labeling methods to supports more
annotation types and new assisting strategies.
•
We design and handle a complex system with multiple components: frontend,
backend, models server, and database. Caching and reducing data transferring
methods are applied to cut down transmission size and time. We also employ
containerizing system components and packaging models to allow easy and
fast deployment as well as scaling up. Besides that, our deployment solutions
can handle multiple concurrent requests so that it allows access for multiple
users at once with parallel calls to platform features and deep learning model
predictions.
1.4 Thesis outline
Our thesis contains 8 chapters:
•
Chapter 1: Introduction
Chapter 1 provides a brief summary about existed annotation platforms and
the important role of the labeling platform for the artificial intelligence in-
dustry. The chapter also mentions the gap of current annotation platforms
in terms of suggestions based on defined class as well as our orientation to
address this issue.
•
Chapter 2: Related work
Chapter 2 introduces some popular annotation platforms in detail as well as
previous works of referring expressions. Moreover, the chapter also gives a
brief history of multi-modal fusion and the prior work that we inherited.
•
Chapter 3: Creating a web application using ReactJS
Chapter 3 gives a brief introduction about ReactJS which is the main frame-
work we use to implementing our annotation platform. We give an overview of
6
the main features and components in a React application, some of the library
limitations, and our use cases in the project.
•
Chapter 4: Front-end architecture
Chapter 4 gives a detailed description of the front-end part of the platform. We
list out problems of project structure, state management, and communication
between components. Then, we describe strategies and methods applied to
address those problems.
•
Chapter 5: Back-end architecture
Chapter 5 gives details about our server infrastructure. First, we introduce
Flask framework for implementing API. Then we describe solutions and tech-
niques that we use to handle problems relating to storage, deployment, and
model serving.
•
Chapter 6: Method
Chapter 6 describes the algorithm that we integrate into the platform. It
includes details algorithm of prior work, our modifications based on experi-
ments, and our strategy for applying image referring segmentation into video
referring segmentation.
•
Chapter 7: Smart Annotation Platform
Chapter 7 present a detailed view of our annotation platform which including
infrastructure, description of features, and algorithm applied. We also present
the annotation workflows in our platform and give reasons for chosen workflow
design.
•
Chapter 8: Conclusion
Chapter 8 summarizes our works for platform architecture and the result of
our algorithms. Finally, we propose future work to develop our system to
enhance the user experience besides improving the effectiveness of supported
algorithms.
7
Chapter 2
Related work
In Chapter 2, we first introduce previous Referring Expression Compre-
hension and Segmentation methods. Moreover, we also introduce brief
history of multi-modal fusion, and the prior work that we inherited. Be-
side that, we give a review about existing annotation platforms with their
main features along with the application of labeling automation in reduc-
ing manual work.
2.1 Referring Expression Comprehension
and Segmentation
2.1.1 Task overview
Referring Expression Comprehension or Segmentation aims to localize objects
in the image by using natural language expressions. The expressions often include
object description and the object’s location, and the relationship between object.
In Figure 2.1, the query describe the appearance of the man such as "black man"
or "black suit" which is used to distinguish with another man in image.
In Video Referring Expression Comprehension or Segmentation fields, there are
two types of queries: single frame query and full-video query. The first type con-
tains information about an object in a specific frame (lacking action information),
and the second type contains information about that object through multiple
frames. In Figure 2.2a, the query describe the object in the frame, whereas in
8
Query: A black man in a black suit.
Figure 2.1: Sample in G-Ref dataset
Figure 2.2b, it requires multiple frames to know which "parrot" is referred.
2.1.2 Methods overview
Many existing works with a supervised approach can divide into 2 main groups:
the two-stage the one-stage [5]. Two-stage methods find the best match object
from the list of objects predicted by other models. In contrast, one-stage methods
try to exploit the relation between linguistic and visual information and directly
predict the bounding box or mask.
Two-stage approach
The common pipeline for the two-stage approach includes i) using an object
detection/segmentation system to find all objects in an image ii) find the best
match objects by using graph, parsing tree of the sentence, etc.
Graph-based approach In referring expression, query describes the object’s
attribute and gives location information about the objects and the relationship
between them to avoid ambiguity. So, to construct the relationship between objects
in an image, [6], [7] uses a graph-based method to exploit this information.
9
Query: a parrot held by the right hand of a person
Query: a parrot sitting then being picked up and play fighting with another parrot
Frame 1 Frame 85
a)
b)
Figure 2.2: a) Single-frame query b) Full-video query
Wang et al. introduced a language-guided graph attention network (LGRAN)
[6], consisting of 3 parts: language-self attention module, language-guided graph
attention module, and matching module as in Figure 2.3. In the language-self
attention module, the model classifies each word of the sentence into 3 types of
information: subject, intra-class relationship, inter-class relationship. Then, each
type is used in the language-guide graph attention module to construct the graph.
In particular, subject-part is used for node attention, intra-class relationship and
inter-class relationship part are used for edge attention. The node attention em-
phasizes the matched object; the edge attentions create the representations based
on the intra-class relationship and inter-class relationship. In the end, the matching
module will calculate the matching score between 3 representations with 3 parts
of the sentence and choose the best one.
Parsing tree of the sentence Language features usually be simplified by using
sentence-level features or phrase-level features without considering the sentence
structure. Some works try to improve this by applying attention mechanisms
to separate the sentence into multiple parts for multiple purposes such as object
matching, location matching, relation matching [8]. However, it still does not meet
the way humans understanding and can cause the critical problem - biased towards
10
Figure 2.3: Source: Neighborhood Watch: Referring Expression Comprehension via Language-guided Graph Attention Networks
Figure 2.4: Source: Learning to Assemble Neural Module Tree Networks for Visual Grounding
learning a certain visual language model rather than visual reasoning [5].
Liu et al. proposed Neural Module Tree Networks (NMTree) [9] for visual
grounding to address this issue. Firstly, they use the Dependency Parsing Tree to
parse the sentence then extract the feature for each node with Bidirectional Tree
LSTM. Secondly, they use three modules: Single, Sum, and Comp, to accumulate
the score of the object in the bottom-up direction. In detail, The Single module
uses to calculate scores for each region at leave nodes and root node, whereas Sum
and Comp module is used for the reasoning process when going from leaves to root.
Scene-graph approach To combine the benefit of parsing tree and graph-
based method, Sibei Yang et al. proposed Scene Graph guided Modular Network
11
Figure 2.5: Source: Graph-Structured Referring Expression Reasoning in The Wild
[10]. First, the model will create graphs for both the image and language, in terms
of language, language scene-graph, where a node and an edge of the graph represent
an object and a relation between two objects. In terms of the image, they build
an image semantic graph, where a node is objects detected from [11]. Then, the
directed edge is computed using a spatial relationship between the object’s two
objects and visual features. The second step is the reasoning process, where the
model uses the language-scene graph to guide the image semantic graph in the
bottom-up direction as same as parsing tree-based method.
Recent works in Referring Expression Segmentation with a two-stage approach
inherit the works of Referring Expression Comprehension task but replace detection
module with segmentation module as [12], [13]
One-stage approach
Although the two-stage approach produces reasonable and interpretable results,
it still has some limitations. Firstly, the upper bound performance depends on the
object detection/segmentation system. Secondly, the inference time is slower than
the one-stage approach and hard to process in real-time. Therefore, the one-stage
approach is chosen by [14]–[16], and some of them produce higher results than
many two-stage methods previously.
Due to predicting directly from the visual features, multi-level features and
multi-modal fusion are widely adopted in one-stage methods. Zhengyuan Yang
et al. proposed One-Stage Approach to Visual Grounding [17], which contains 3
12
Figure 2.6: Source: A Fast and Accurate One-Stage Approach to Visual Grounding
parts: extract features by using features pyramid network; fusion module by con-
catenating language features, visual features, spatial coordinates; and prediction
module. The detail can be seen in Figure 2.6
Concatenating visual features and language features is not an efficient way [14],
so many works use multi-fusion based on bilinear pooling which is outperform
previous method. Moreover, without using fusion after features encoding as many
previous works, Feng et al. proposed a new strategy for multi-modal fusion which
illustrate in Figure 2.7. Particularly, instead of fusion language features and visual
features at the decoding process, the authors proposed a fusion method at the
encoding process [15].
Video Referring Expression Comprehension and Segmentation
There are two main problems in video referring expression: video understanding
and inconsistency prediction between two frames. Feng et al. proposed Decoupled
Spatial-Temporal Graphs [16] to address the first issue. Concretely, they build
temporal connection by using the similarity of objects between two frames, then
compute the features based on that graph. Therefore, the object features will
contain the action information throughout the video.
Seo et al. introduced Unified Referring Video Object Segmentation Network,
which adopts the Memory Attention module of STM Network in terms of inconsis-
tency prediction. Thereby, the prediction of current frames will be guided by the
previous predictions and improve the consistency throughout the video. In this
13
Figure 2.7: Source: Encoder Fusion Network with Co-Attention Embedding for Referring Image Segmentation
Figure 2.8: Source: Decoupled Spatial-Temporal Graphs
thesis, we also focus on second issue by proposed a new way to combine mask in
multiple frames without changing our model, which is used for single image.
2.2 Multi-modal fusion
In tasks that have multiple input types, such as Visual Question Answering
(image and text), Referring Expression Comprehension (image and text), Video
Classification (image and audio), they need a mechanism to fuse these inputs into
a multi-modal representation before prediction. A simple way is concatenating the
features then passing through the linear model. However, many previous works val-
idated that bilinear pooling can outperform the traditional linear model, especially
14
Figure 2.9: Source: Unified Referring Video Object Segmentation Network
in the Visual Question Answering task [2].
The bilinear pooling is also known as the full outer product of vectors.
a⊗ b =
 a1b1 a1b2 . . . a1bn
a2b1 a2b2 . . . a2bn ...
... . . .
...
anb1 anb2 ... anbn
 In the VQA tasks, if the image features and text features have 2048 dimensions,
then the multi-modal features will have 20482 dimensions. This requires a lot of
learnable parameters for passing through a fully connected layer to predict 1000
classes.
Therefore, Multi-modal compact bilinear pooling (MCB) [18] was proposed to
project the outer product to lower-dimension space to reduce the number of pa-
rameters and computational cost. In particular, the features of the outer product
after reduction have 16000 dimensions. However, that number still consumes a big
memory, and computational cost, so many other approaches proposed to address
this issue, such as Low-Rank Bilinear Pooling (MLB), Multi-modal Factorized Bi-
linear Pooling (MFB), Multimodal Tucker Fusion (MUTAN) [19]. In the previous
work that we were inspired by, they chose MUTAN as a Bilinear Fusion module.
The detail of the methods will be mentioned in Chapter 6.
15
2.3 Referring Image Segmentation via
Cross-Modal Progressive Comprehension
Huang et al. proposed Cross-Modal Progressive Comprehension [20] to mimic
how humans can detect the object referred to by a sentence in an image. In
particular, they claim that people will first locate the object mentioned in the
sentence, then consider the relationship between objects to select the best match -
which is also similar to the two-stage methods. However, to bring it to the one-stage
approach, they introduced two modules: i) Entity Perception, ii) Relation-aware
Reasoning.
The first module is used to fuse visual features and linguistic features of entity
words and attribute words. The latter is used to exploit the relation between
the objects. For example, in Figure 2.10, "man" and "white frisbee" are used to
fuse with visual features. However, there are three men in the image; therefore,
"holding" is used to build the connection between "frisbee" and "man" then guide
the model to select the referred object in the image. We will describe concretely
two modules in Chapter 6.
Figure 2.10: Source: Referring Image Segmentation via Cross-Modal Progressive Comprehension
16
2.4 Annotation platform
2.4.1 Overview
Data annotation tasks is playing an important role in the development of deep
learning algorithm. However, the task of labeling data to create dataset that is
sufficient in both quantity and quality is often require much of human resources and
very intensive. That is where the annotation platforms come to help. Annotation
platforms facilitate the labeling tasks by providing features that simplify, speed
up, and improve the quality of produced output data.
Figure 2.11: Key elements of Data Annotation Tools
Source: https://www.cloudfactory.com/data-annotation-tool-guide
There are a lot of competitive annotation platforms that give a wide set of choices
for companies to consider on the annotation task. The variations of annotation
platforms come due to data types, and software distribution.
Data types supported by annotation are diverse as the data types models run
on. For natural language processing, tools that support text data for labeling
17
classification, tagging, translation are required. In computer vision, image and
video need platforms with appropriate support for loading and playing the provided
data, with additional interactions between video frames. These requirements also
vary to other data types such as audio or time series.
Annotation platform solutions mainly run on web environments, whereas some
come as desktop software which need installation. There are also options in choos-
ing paid or free solution annotation tools. Paid tools usually provide user-friendly
interfaces with lots of integrated features that are ready to use and human labeling
service is also available. Free solutions such as open-source or freeware would need
more configuration and system requirements to run on but they also provide a
higher level of customization for labeling tasks requirements.
Some popular annotation platforms for computer vision are supervise.ly, label-
box.com, V7, playment.io, VoTT, SuperAnnotate.com, CVAT, etc.
2.4.2 Annotation platform features
An annotation platform provides users an interface to working on the labeling
tasks, and comes with a standard set of tools for annotating, and data management
functions. As we focus on video object segmentation in this thesis, we will con-
centrate on analyzing features of a computer vision supported annotation platform
for images and videos.
Dataset management
Annotations start with data when you need to upload or import raw data which
you need to annotate. Data management functions include structuring data, and
giving the ability to search, filter, and delete. After importing and processing data
using labeling methods. You might need to export your annotations into model
input so that you can feed the model for training or evaluation. Most annotation
platforms support multiple standard dataset output formats such as YOLO, MS
COCO, Pascal VOC, etc.
Labeling methods
A common annotation tool for videos and images consists of view handling and
drawing functions. View handling provides ways to change views of the image, such
18
Figure 2.12: Data management screen shot from SuperAnnotate.com
as zoom in/out, dragging the viewport, etc., and additional settings to enhance
image quality, such as adjusting the image’s contrast and brightness, etc.
Drawing tools includes methods and interactions for giving labels to the data.
Specifically for video and image data, there are label types such as: bounding boxes,
polygons, polylines, classification, 2-D and 3-D points, or segmentation (semantic
or instance), transcription,. . . Annotators usually use computer mouse gestures as
input for generating annotations with additional keyboard shortcuts.
Other features
Some platforms provide additional features such as task assignment, progress
tracking for cooperation of annotator teams, or consensus models and metrics for
data quality control.
2.4.3 Data labeling automation
Annotating from scratch manually by annotators is such a tedious task. Com-
mon labeling tasks such as object classification for animals like cat, dog, fish, . . .
does not involve a lot of thinking for human labelers. However, it is very boring
and requires lots of repetitive work. As the dataset size scales up to thousands or
19
Figure 2.13: Labeling for video tool from supervise.ly
millions of images and frames, the labeling process takes up a huge amount of time
and a large number of annotators to work on. Therefore, many algorithms are ap-
plied into annotation tools for automation labeling which assist human annotators
to improve their accuracy and speed.
The algorithms involved aid the annotation process at different levels depending
on the applied algorithms. At lower levels, classical computer vision algorithms are
integrated as assisting tools. For example, in task object segmentation, superpixels
are applied to fragment the image into clusters of highly related pixels so that
annotators can pick regions of pixels at the same time for better edge precision.
With videos, interpolation models are applied to mitigate the intensive manual
effort for every single frame in each video.
At higher levels of automation, deep learning and machine learning models are
employed to significantly reduce human efforts for annotation. By using AI, anno-
tation tools minimize human interactions and work by supporting weaker annota-
tion types or pre-labeling data.
Weak annotation support means that users will provide simple interactions
as guidance to models to predict the annotation itself. In detail, instead of requir-
20
Figure 2.14: Weak annotation by 4 clicks using Deep Extreme Cut
ing precise annotations at pixel level, annotators are only required to give weak
annotations as input for models to produce the full complex annotation such as
segmentation masks. Many works show that using weak annotation reduces lots of
effort during labeling. COCO annotator [21] takes advantage of Deep Extreme Cut
(DEXTR) [22] It only requires the annotator to make only 4 extreme clicks (top,
left, right, and bottom most points) for each object to produce a segmentation
mask as shown in Figure 2.14.
PathTrack [23] introduces simple interaction for annotating tracks of object
masks in videos by using the cursor to follow the object in the video’s duration
before the model generates the annotation for the full track itself. It is shown
that the annotation process is only 33% slower than watching the video normally.
Platforms that support annotation for object tracking in videos (LabelBox, Play-
ment) applied deep learning models such as BubbleNets [24], Frame guidance [25]
to produce key frames suggestion and interpolation to let users annotate only a
small fraction of frames while keeping a high accuracy of the track.
Pre-labeling takes advantage of models that are pre-trained on large existing
datasets. Annotation tools will run these models in advance to generate annota-
tions with labels depending on user-defined settings. Then, human annotators will
only need to review those generated annotations to refine or delete incorrect anno-
tations. This approach works well for commonly seen object classes when they are
21
Figure 2.15: Pre-labeling tool of playment.io allow users to set the confidence score threshold for filtering detected object bounding boxes
already included in trained datasets. With good pre-trained models, users do not
have to edit predicted annotations or add manually annotated labels too much.
Figure 2.15 shows an example of pre-labeling feature on playment.io that allow
users set threshold to select annotations from model predicted ones.
Another strategy which provides incremental learning progress for automation
labeling is using human-in-the-loop (HITL) and active learning workflow. First,
the model requires a small number of examples as a “seed” dataset to train on.
Then, depending on the model trained on the “seed” dataset, it will request ad-
ditional manually labels on unlabeled data points based on higher uncertainty
measurements which mean that it is unsure about.
The model will continue to learn from those new labels and update to improve
its performance. By having the human iteratively teach the model, it’s possible to
make a better model in less time with much fewer manual labels. When the model
reaches a desired performance level, it will be used to predict annotation labels on
remaining unlabeled data. The active learning flow is illustrated in Figure 2.16.
22
Figure 2.16: Active Learning basic flow
Source: Data Annotation Using Active Learning With Python Code
2.4.4 Conclusion
With our goal to incorporate AI-based assistance to users in visual data labelling,
especially with segmentation tasks, we present in this chapter existing work on
Referring Expression and Segmentation. Then we briefly present the main features
of annotation platforms. These are the starting points for our proposed solutions to
develop our Smart Assistance for Annotation Platform presented in the following
chapters.
23
Chapter 3
Creating a web application
using ReactJS
In this chapter, we introduce ReactJS library and its core components
through explanation, example, and sample code snippets. We also describe
limitations of React dataflow and represent our use cases that taking ad-
vantages of React hooks.
3.1 React Library
3.1.1 Overview of React library
React is a JavaScript library for building user interfaces developed by Facebook
Inc. It was open sourced in 2013 and released version 17.0.2 in 2021. Based on a
survey conducted by Stack Overflow [26], ReactJS is the second most popular web
framework with 35.9% of respondents standing after jQuery with 43.3%. However,
the survey also shows that React is still increasing gradually through the years
while jQuery is slowly losing.
React provides declarative views to write UI components by allowing them
to manage state and inject data right into views in JSX. Additionally, React is
component-based in which each component is encapsulated to manage its own
state, lifecycle, and then many components are composed to make entire complex
UIs.
24
3.1.2 React features
In React, render logic and UI logic are coupled in one place so that manipulating
the state can directly reflect in UI components and reverse. React also has a
mechanism to efficiently update and render just the right components when data
changes.
Component-based approach provides developers with the ability to divide a big
application into smaller components so that users can easily handle each smaller
component with its own UIs element and functionalities. This approach also sup-
ports the flexibility of the gradual development of components. In which developers
have the capability to replace or edit one component without affecting the other
components. New components can also be integrated. Additionally, existing com-
ponents are reusable in different views without the need of reimplementing the
same lines of code to reduce the workload.
3.1.3 React web environment
React is a web framework so the application written with React is built into
HTML, JavaScript, and CSS files and runs directly on the browser. Therefore,
React web applications do not depend on the operating system but can run inside
browsers of any device such as desktop, tablet, mobile. This provides cross-platform
support without any further installation from users and allows the ability to ex-
perience the annotation tools in different environments. Especially giving good
experiences on touch-supported devices for annotation tasks which include many
gestures and interactions.
3.1.4 Extendability using JavaScript libraries
React is written in JavaScript which is the most popular programming language
[26]. Therefore, it adds to the power of React by allowing the use of a huge
number of existing external JavaScript libraries directly. With those libraries,
many features and functionality can be integrated easily through installation and
import.
In addition, due to its popularity, React also has a large number of React-based
libraries which are 74,653 dependents shown on the npm package management
25
website [27]. As a result, many different React libraries cover multiple features
of web applications and are readily available for use without re-implementing for
faster development.
3.2 React Component
The basis of React is components. Components let us split the UI into indepen-
dent, reusable pieces, and handle each piece in isolation.
In general, each React component consists of its props, state, lifecycle, logic
functions, and return React element describing what should appear on the screen.
3.2.1 Function and Class Components
Each React component can be a JavaScript function or class component which
is extended from the React.Component base class. Both Function and Class com-
ponents are equivalent and have similar features in React.
Syntax for defining a Function component:
1 function Welcome(props) {
2 return <h1>Hello , {props.name} </h1>;
3 }
Listing 3.1: A JavaScript function which accepts a single “props” object argument
with data and returns a React element.
Syntax for defining a Class component:
1 class Welcome extends React.Component {
2 render () {
3 return <h1>Hello , {this.props.name} </h1>;
4 }
5 }
Listing 3.2: A React class component extends from React.Component class
This component is called a Class component because it extends from the Re-
act.Component base class which also accepts props accessed through this.props
and a render method which returns the UI elements. Both types of components
defined give the same result when rendering, which is a text enclosed in a h1 tag
depending on the “name” props which is “World” in this case.
26
Figure 3.1: "Hello World" h1 element rendered by code snippet in Listing 3.3
1 const element = <Welcome name="World" />;
2 ReactDOM.render(element , document.getElementById(’root’));
Listing 3.3: Render a React component to DOM
Class components, which are inherited from React.Component, have included
methods to handle and manipulate component lifecycles from mounting to un-
mounting from the DOM (see Section 3.2.2). Whereas the function component
supports managing component state and lifecycles through hooks (see [TODO]
React Hooks) which are more flexible and simpler to use.
3.2.2 Component lifecycles
When defining a component as a Class component, it inherits several “lifecycle
methods” that we can override to run code and change the behavior of the compo-
nent at particular times in the process. Overall, each component lifecycle consists
of 3 stages which are: Mounting, Updating, and Unmounting as illustrated in Fig-
ure 3.2. In each of these stages, there are methods being called in order. In this
section, we focus on commonly used lifecycle methods.
Figure 3.2: Class component common lifecycle
Source: https://projects.wojtekmaj.pl/react-lifecycle-methods-diagram/
27
Commonly Used Lifecycle Methods
render(): This is the only required method in a class component. When called,
it returns one of the following types based on data of this.props and this.state of
the component:
•
React element: typically created via JSX. It can be naive HTML elements or
user-defined React elements that instruct React to render a DOM node.
1 render () {
2 return <h1>Hello , {this.props.name} </h1>;
3 }
•
Arrays and fragments: allow return of multiple elements from the render
method. Noting for array of elements, each element must have a unique key
among the array so that React can detect changes and only re-render elements
with new updates.
1 constructor () {
2 this.state = {
3 todoList: [
4 { id: ’1’, name: ’wash clothes ’ },
5 { id: ’2’, name: ’buy food’ },
6 { id: ’3’, name: ’do homework ’ },
7 ]
8 }
9 }
10 render () {
11 return (
12 this.state.todoList.map(task => <li key={task.id}>{task.
name}</li>)
13 )
14 }
•
String and numbers: they are rendered as text nodes in DOM
•
Boolean or null: render nothing. These are used in components mainly for
handling logic or conditional rendering
1 constructor () {
2 this.state = {
28
3 isHidden: true
4 }
5 }
6 render () {
7 return (
8 !this.state.isHidden && <span>This is a secret text
message!</span>
9 )
10 }
Listing 3.4: State “isHidden” is currently true so that the conditional statement
returns false. Therefore this component does not render any DOM element.
constructor(): This method is only required to implement if we need to ini-
tialize component state and bind methods.
If implementing constructor(), we need to call the React.Component base class
constructor by calling super(props), otherwise this.props will be undefined and
hence cannot get passed props in the component.
1 constructor(props) {
2 super(props);
3 // Don’t call this.setState () here!
4 this.state = { counter: 0 };
5 this.handleClick = this.handleClick.bind(this);
6 }
componentDidMount(): This method is invoked right after the component is
mounted (inserted into the tree). The main purpose of this method is to load data
from remote endpoints or initialize subscriptions. For subscriptions, we need to re-
member to unsubscribe in the componentWillUnmount method to prevent uncontrolled
scenarios.
1 componentDidMount () {
2 fetch("https :// api.example.com/items")
3 .then(res => res.json())
4 .then(
5 (result) => {
6 this.setState ({
7 isLoaded: true ,
8 items: result.items
29
9 });
10 },
11 (error) => {
12 this.setState ({
13 isLoaded: true ,
14 error
15 });
16 }
17 )
18 }
componentWillUnmount(): The componentWillUnmount method is called right
before the component is unmounted and destroyed. It is necessary to do the clean
up here. Such as invalidating timers, interval functions, or unsubscribing subscrip-
tions initialized in the componentDidMount method.
1 class CountUp extends React.Component {
2 constructor () {
3 this.state = {
4 count: 0
5 }
6 }
7
8 componentDidMount () {
9 this.countingInterval = setInterval (() => {
10 this.setState(state => ({
11 count: state.count + 1
12 }))
13 }, 1000);
14
15 this.setState ({ countingInterval })
16 }
17
18 componentWillUnmount () {
19 clearInterval(this.countingInterval)
20 }
21
22 render () {
23 return (
24 <div>
25 {this.state.count} seconds
30
26 </div>
27 )
28 }
29 }
Listing 3.5: This is an example of a full React class component with basic lifecycle
methods
3.2.3 Component props
this.props is an object that contains the properties passed down from the parent
of this component. Props data can be used to render UI content or source for
computing component’s own state. There is one special prop, this.props.children,
which is defined by JSX as elements inside the enclosing tag when calling this
component. An important rule in React is that components must not alter the
props they receive.
For example, this Parent component renders the Children component with 2
props which are:
•
Props number with value 10
•
Props children is a div element with text inside
1 class Parent extends React.Component {
2 render () {
3 return
4 <Children number ={10} >
5 <div>Div as children props </div>
6 </Children >
7 }
8 }
In the Children component, two passed props can be accessed through this.
props.number and this.props.children
1 class Children extends React.Component {
2 render () {
3 return (
4 <div>
5 <div>Number: {this.props.number} </div>
31
6 {this.props.children}
7 </div>
8 )
9 }
3.2.4 Component state
In contrast to props, the component state is managed by the component on its
own and can be changed over time. The state is user-defined and is a JavaScript
object. The state only stores values that are used for rendering or data flow. Other
variables can be defined directly as class fields on the component instance. The
component’s state can be initialized in the component constructor method and be
updated through the setState method.
1 constructor () {
2 this.state = {
3 count: 0
4 }
5 this.setCount = this.setCount.bind(this);
6 }
7
8 setCount(newCount) {
9 this.setState ({ count: newCount })
10 }
Listing 3.6: In constructor, we initialize the state variable count with 0. Then in the
setCount function, we update this.state.count to the value of argument newCount.
State updates may be asynchronous
React may batch multiple setState calls into a single update for performance.
Therefore, this.props and this.state are updated asynchronously and we should
not rely on their values for calculating the next state. However, there is setState
method form that accepts function form which receives two arguments (state,
props) to calculate the new state.
1 this.setState ((state , props) => ({
2 counter: state.counter + props.increment
3 }));
32
3.2.5 Handling events
Handling events in React is similar to handling events in DOM elements. How-
ever, there are two main differences:
•
React events are named using camelCase
•
We pass a function handler to JSX instead of a string
1 <button onClick ={ activateLasers}>
2 Activate Lasers
3 </button>
An event handler function will receive an argument e which is a synthetic event.
You call methods of e to prevent default behavior, cancel bubbles,. . . or access the
target element to get new values.
1 function Form() {
2 function handleSubmit(e) {
3 e.preventDefault ();
4 }
5
6 function handleChange(e) {
7 console.log("New value", e.target.value)
8 }
9
10 return (
11 <form onSubmit ={ handleSubmit}>
12 <input onChange ={ handleChange}/>
13 <button type="submit">Submit </button>
14 </form>
15 )
16 }
Listing 3.7: Calling e.preventDefault() to prevent the default submit behavior of
a form and accessing the target element using e.target
If we use class components and define handlers as class methods, we need to
bind this to the handler first in the constructor. Or else when the handler is called,
this will be undefined and you can not access state, props, or other class methods.
1 constructor(props) {
33
2 super(props)
3 this.state = { value: ’’ }
4 this.boundHandler = this.boundHandler.bind(this);
5 }
6 boundHandler(e) {
7 this.setState ({ value: e.target.value })
8 }
9 unboundHandler(e) {
10 this.setState ({ value: e.target.value }) // Error: this will
be undefined
11 }
Listing 3.8: The boundHandler which we bind “this” in constructor will work well.
The unboundHandler will raise an error because “this” will be undefined. An alternative way of binding is to define handlers in an arrow function form which
automatically binds this to the function.
1 handleClick = () => {
2 console.log(’this is:’, this);
3 }
4 render () {
5 return (
6 <button onClick ={this.handleClick}>
7 Click me
8 </button>
9 );
10 }
You can pass additional arguments to the handler using one of the two ways.
This is applicable in cases such as rendering a list and needing the id of the row to
perform an action.
1 <button
2 onClick ={(e) => this.deleteRow(id , e)}
3 >
4 Delete Row
5 </button>
6 <button
7 onClick ={this.deleteRow.bind(this , id)}
8 >
9 Delete Row
10 </button>
34
3.3 React data flow
3.3.1 Top-down data flow
In React, neither parent nor child components can directly access the state of
the component. Only the component owns its own state and can set the state
directly. A component can choose to pass its state down as props to its child
component. The child component only knows about the props it receives without
knowing whether it comes from and it cannot alter the props (the rule is mentioned
in Subsection 3.2.3).
The data flow in React is illustrated in Figure 3.3. This is commonly called “top-
down” or “unidirectional” data flow. Any state is always owned by some specific
component, and any data or UI derived from that state can only affect components
below them in the tree.
Lifting state up
Despite that React uses top-down data flow, we still need a mechanism to reflect
the changing data to the parent component and share state between sibling com-
ponents. To solve this problem, we use a pattern to lift the state up level-by-level
to their closest common ancestor.
In addition to the value passed through props, we pass a handler which has the
responsibility to receive changes in child components and update the component’s
state accordingly.
This pattern helps maintain a “single source of truth” which is the component
that owns the state and ensures the top-down data flow when the state from
the source is passed down along the tree to propagate new changes to its child
components.
However, the lifting state involves more writing codes for passing both value
and handler down along the tree, especially when there are deep levels of nested
components. This arises with more effort and potential errors when changing code.
We will discuss and propose a solution to this problem in our application in Chapter
4.
35
Figure 3.3: Top-down data flow and lifting state up combined into full React data flow
Source: ReactJS Training: Understanding React and TypeScript
3.4 React Hooks
3.4.1 Hooks overview
Class components provide us with full control on component state, props, and
lifecycle. However, there are some limitations in the way and use of class compo-
nents:
•
Hard to reuse stateful logic between components: it can be solved by using
render props (props to define render logic) or higher-order components which
are integrated by wrapping up the current component with nested layers of
components providing features. But this approach causes such a situation
named “wrapper hell” of components when it becomes a complicated structure
and requires restructuring when you want to add or remove some components.
•
Complex components become harder to understand: you need to call all re-
quired functions in the same lifecycle methods but they are sometimes unre-
lated and become hard to follow. In contrast, some functions and subscriptions
36
that need clean up logic are split apart into two places in componentWillMount
and componentWillUnmount, making it hard to maintain and might cause incon-
sistency when editing one.
•
Class confusion: starting to implement a class in JavaScript might be difficult
by understanding the concept and how this works. Therefore, changing from
class component to function component helps mitigate this problem. Also, the
React team found that function components have more potential in optimiza-
tion.
For those reasons, React introduces hooks. Hooks allow us to use class compo-
nent features in a different way which is simpler to use, more structural, and has
potential for optimization. You can start using Hooks right away without rewriting
any existing codes.
3.4.2 Using the State hook
useState hook is the alternative to this.state and this.setState in the class
component which allow you to create and manage a state variable.
We can start using the useState hooks immediately in the function component
with the hook import from the React library.
1 import React , { useState } from ’react ’;
2
3 function Example () {
4 // Declare a new state variable , which we’ll call "count"
5 const [count , setCount] = useState (0);
The declaration contains 2 parts: the return value and argument to the useState
hook. The hook receives one argument which is the initial value of the variable
which is count in this example. It will initialize a new variable count with value 0.
This argument can be in any datatype from number, string, or object. The hook re-
turns an array with two elements: the variable and the set function which we obtain
through JavaScript array destructuring. The variable count can be used directly
in composing JSX elements or as parameters for further calculating. This vari-
able will get updated and cause re-render or re-calculation whenever the setCount
function is called to set a new value.
37
1 return (
2 <div>
3 <p>You clicked {count} times </p>
4 <button onClick ={() => setCount(count + 1)}>
5 Click me
6 </button>
7 </div>
8 );
Listing 3.9: The variable count is used in the JSX element to show the number of
click(s) and can be updated through clicking the button with attached setCount
execution
3.4.3 Using the Effect hook
The useEffect hook adds the ability to perform side effects such as data fetch-
ing, subscriptions, or manually changing the DOM in the function component. It
combines and serves the same purposes as componentDidMount, componentDidUpdate,
and componentWillUnmount in class components.
The most basic useEffect can be declared with one function argument.
1 useEffect (() => {
2 document.title = ‘You clicked ${count} times ‘;
3 });
The function in the above code will be called at every re-render of the component.
We can control this through the second parameter which accept a list of variables.
The function in the first argument is only called whenever one of the elements in
the second array argument is changed. This will reduce the number of executions
and improve the performance of the component. A special argument is an empty
array [ ] which tells the hook to run the effect and clean it up only once (when
component mount and unmount).
1 useEffect (() => {
2 document.title = ‘You clicked ${count} times ‘;
3 }, [count ]);
Listing 3.10: This adds a small modification to the previously defined hook. The
function will only be triggered when the variable is changed.
38
To define the clean up part of an effect, for example, equivalent to unsubscribing
with the componentWillUnmount method of the class component, we return a function
that includes all clean up code.
1 useEffect (() => {
2 function handleStatusChange(status) {
3 setIsOnline(status.isOnline);
4 }
5 ChatAPI.subscribeToFriendStatus(props.friend.id,
handleStatusChange);
6 // Specify how to clean up after this effect:
7 return function cleanup () {
8 ChatAPI.unsubscribeFromFriendStatus(props.friend.id,
handleStatusChange);
9 };
10 });
Clean up part of the useEffect hook will be called when the component unmount
and clean up the effect in the previous render (or changes in the case with array
argument controlling) before running the effect next time.
Application: optimizing page load using useEffect hook and react-router
library
•
Problem: A page in the application might contain many small components
and they add up a significant amount of time for loading the page. However,
for pages in the application that include pagination or other route parameters,
whenever changing one parameter using normal redirect and navigation will
take a lot of time to reload the page and force users to wait for a period of
time.
•
Solution: We combined features of the useEffect hook and react-router library
which is used for controlling routing in the application to enhance the perfor-
mance. Instead of redirecting to a new page with a route of new parameters
for every change. We decouple the task into two stages:
1. Use react-router library to push the new route to the history object which
changes the page URL without reloading: First we get the history object
using the useHistory hook provided by the react-router library. Then
39
whenever users change the page, we call the history.push method to alter
the page’s URL.
1 const history = useHistory ()
2 const handlePageChange = (event , newPage) => {
3 history.push(‘/datasets/dataset=${datasetId }?page=${
newPage}‘)
4 }
2. useEffect hook to observe the page parameter in the page URL and call
function to get data whenever there are changes: We have a useEffect
hook which calls a function to load new data corresponding to a new page
parameter. This function is also called by default when the component
mounts to get initial data.
By splitting into two small stages instead of direct navigation, we prevent reload-
ing of a full page which contains components that do not depend on paging but only
re-render components that relate to displaying data which depend on the current
page. This helps us reduce waiting time and create better experiences for users.
3.4.4 Conclusion
In this chapter, we present the main techniques with ReactJS using its library
and core components, together with our our use cases to take advantages of Re-
act hooks, to develop web application. We present the Backend and Frontend
architecture and technical solutions in Chapter 4 and Chapter 5, respectively.
40
Chapter 4
Frontend Architecture
In this chapter, we describe our architecture design for the front-end ap-
plication, which aim to be flexible for updating or extending application
functionality through proposed modular structure and global state manage-
ment. Then we solve the problem of communication between components
by an event-based method following observer pattern.
4.1 Overview
In our application there are many pages, some might use different sets of tech-
niques described in Chapter 4. This section will focus on analyzing the main
annotation functions page architecture which consists of a complex combination of
components to illustrate all techniques we applied. Other pages may use some of
the techniques described here and simpler and direct solutions of some problems
for simplicity.
Annotation page consists of many functions including drawing, visualization,
and data management. Additionally, we aim to build a flexible platform which can
be modified or upgraded to change current application behavior or to add more
features. The flexibility of the platform is also in demand for the extensibility of the
platform so it can adapt to new annotation task requirements such as new annota-
tion types, changing prediction modules, or data management strategy. Therefore,
it becomes very complex and hard to control without using appropriate design
and pattern. To build a solution that meets given requirements, we have to due
with appropriate components structure, state management, and communication
between components which are described in following subsections.
41
4.2 Modular structure
4.2.1 Problem
The React application is component-based which allows dividing UI elements
and functionalities into independent, reusable pieces in isolation, and combined
together by the composition model. However, when adding many features, small
components become more complex and introduce more correlation dependencies
between components, especially in multiple nested layers of components.
When structuring a nested structure, the higher level the component is, the
more logic and code it must contain to accommodate components in lower levels.
It also adds difficulty in splitting up into smaller independent components when
the component grows. And when there are changes, we must go along the full
tree path from that component to the root and down to its children, modifying all
the related code. This slows down the development process, makes it harder to
maintain, and also more prone to errors.
4.2.2 Solution
We replace the nested structure by a new modular structure which does not
stack up components by level of containment but splits up into as small as possible
modules which each module has only one responsibility. For example, splitting the
component that handles bounding box annotation into smaller components: one
handle logic and one handle rendering the bounding box on screen in two different
separate modules. Then all the modules are attached into the core as illustrated
as in Figure 4.1
By using the modular approach, it mitigates the heavy correlation between
components on nested structures. Now, each module (which is also a component)
has its own clear responsibility to handle a specified set of events or interactions.
It is easier for us to control the input and output of each module and hence make
the development and debugging process simpler by not requiring us to understand
and modify other modules.
In Figure 4.1, there are module roots. These module roots have the respon-
sibility to select the right modules to run depending on the current state of the
application. For instance, when a user uses drawing bounding box mode on a video,
42
Figure 4.1: The application is divided into multiple modules and attached to the core.
the mode controller must load the drawing bounding box modules which convert
user interaction into coordinates of bounding box rectangle (as in Figure 4.2) and
the data player modules must load appropriately modules for playing video such
as play, pause, stop, go to frame, . . . Besides, the rendering annotation root then
loads modules to draw rectangles on the canvas. The same rule applied for other
module roots and modules as well.
Another advantage of module-based structure is the flexible and extensible abil-
ity. We can later replace one module with another module to change the behavior
of the application or add more modules to add new features.
In Figure 4.3, we can replace the draw bounding box (rectangle) module by a
draw oval module. This will change the behavior of the application so that all
bounding boxes now are used to render as ovals instead of rectangles. This module
replacement does not affect and requires any modifications of other modules in the
same module root and other modules in the application as well.
43
Figure 4.2: The module root selects the appropriate module to run depending on the current state of the core (in this case “mode”)
Figure 4.3: Replacing module
44
4.3 State management
4.3.1 Problem
React has an unidirectional data flow (described in Section 3.3) helps to manage
the state in the composition of components. A component state is managed inside
that component and passed top-down along the tree through props. It also has
a mechanism to reflect data change from child components to parent components
named “lifting state up”. However, this approach faces problems including incon-
sistency and prop drilling as it brings issues in managing the state and involves
more workload on expanding the application, especially in modular architecture.
With an architecture consisting of many modules and components, a state vari-
able at the root of the tree (core) such as “activeMode” controls which tool is being
used by users (drawing, editing, delete, . . . ) is consumed by almost all modules
in the application. Therefore, this variable must be passed as props to all of these
module roots. These roots might consume this variable directly to decide which
modules should run and continue to pass this variable as props to those modules
that require it. Recursively, this variable still needs to be handled and passed down
to inner components that depend on it. This becomes enormous when there are not
only one but many state variables in one application. These passing down props
routines involve a lot of passing code and also raise difficulty in making changes,
even small modifications such as renaming variables or removing no longer used
state.
Not only does it require passing down state variables, it also requires addi-
tional passing down state handler functions to be able to alter the app state from
child components. These handler functions come with the same problems resisting
changes as state variables along with inconsistency problems when multiple com-
ponents can access the handler and alter the state independently. The handler can
be called in any components on different modules and different components in the
tree, once a handler is called inappropriately, for example with wrong arguments
or might have bugs inside that component causing false value inputting to the
handler, it will change the origin state and all components consuming that state
variable will malfunction or break. It is also very difficult to trace the error or
debug the application because there are many different handlers that can alter the
45
Figure 4.4: Flux architecture workflow
state in different ways. Therefore, raising problems understanding when, where,
how the state in the application was updated and how the application logic will
change when those changes occur.
4.3.2 Solution
To overcome those issues, we decided to centralize state-management using a
simplified flux principles implementation by library zustand. This library requires
little code to start and use hooks so that it can be integrated right into existing
functional components. It also ensures React updating rules on state and props so
it only re-render components only on changes.
Flux architecture has three major parts: the dispatcher, the stores, and the
views (React components). In flux architecture, state variables are centralized at
the Store. These variables are updated by a routine through actions which consist
of action type and payload. Every action is sent to the stores via the callbacks
the stores register with the dispatcher. After the stores are updated themselves in
response to an action, they will emit a change event that the view will observe and
update by the new data from the store accordingly. The view, then, can dispatch
actions to update the store state and complete a unified circular flow of data as
shown in Figure 4.4.
This architecture helps solve both inconsistency and props drilling issues while
maintaining top-down data flow of React. With all the states managed by the
store, it ensures a consistent “global” value of every variable across the application.
Each modification must go through a predefined routine through specified actions
and the store will be altered in a controlled manner. It also solves the props drilling
46
Figure 4.5: Zustand architecture workflow
routine when the store is directly accessible to any modules or components of the
tree without repeatedly passing down props along nested components in a simple
and unambiguous way. Additionally, it can help split logic and data into multiple
stores with each handling a piece of data and concern.
Zustand simplified the flux architecture by unifying actions, dispatch, and store
callbacks functions into one direct function which can be called from the view and
update state in stores. The full Zustand flow is illustrated in Figure 4.5. It still
keeps the advantages of store management and data flow of flux architecture while
requiring less implementation and providing simpler understanding.
The code snippets below give instructions to initialize a zustand store and how
to modify the stored state.
•
Creating store: First, we create the store with the provided function from the
zustand library including one variable count and a handle function increment
to increase count by one. The create function receives one function as an
argument which returns an object defining variables and handler functions.
Two arguments functions set and get allow us to access the current state of
the store inside our handler functions. In the following code snippet, we get
the current value of count using get() and then set it to a new value using
set().
47
1 import create from ’zustand ’;
2
3 const useCountStore = create ((set , get) => ({
4 count: 0,
5 increment: () => {
6 const { count } = get();
7 set({ count: count + 1 });
8 }
9 }))
10
Listing 4.1: Creating zustand store
•
Accessing state and modifying state: The create function will provide us a
hook to access variables and functions in the store right in our React compo-
nents. We use a selector to access specified variables or functions in the state.
The value of count is displayed on the screen while attaching the handler
function
1 function Counter () {
2 const count = useCountStore(state => state.count);
3 const increment = useCountStore(state => state.
increment);
4
5 return (
6 <div>
7 <h1>count: {count} </h1>
8 <button onClick ={ increment}>+1 </button>
9 </div>
10 );
11 }
12
Listing 4.2: Access and modifying state
increment to the onClick event of button “+1”. Whenever the button is clicked,
the increment function increases the value of count by one and hence makes the
component Counter re-render with a new value of count.
•
Asynchronous handler: zustand store supports asynchronous handler functions
directly without using additional middleware as in other state management
48
libraries such as Redux. We can simply define an asynchronous handler and
await for an asynchronous routine such as fetching the initial value of a variable
from the server or returning a promise.
1 const useCountStore = create ((set , get) => ({
2 count: 0,
3 loadCount: async () => {
4 await fetch(’https :// SERVER_URL/api/count’)
5 .then(res => res.json())
6 .then(value => set({ count: value }))
7 .catch(error => { console.log(error) })
8 }
9 }))
Listing 4.3: Asynchronous handler function
•
Zustand store also supports additional features such as persisting data to stor-
age, working with redux devtools, integration with React context and more.
4.4 Event based communication between
modules
4.4.1 Problem
We solved the state management problem using a centralized zustand store. Now
the child components can access the state variables directly and also call update
functions directly without the tediousness of going along through nested layers of
components. This solves the vertical communication problem. However, there are
still unresolved issues of horizontal communication between sibling components
and modules.
As illustrated in figure Figure 4.6, two modules that receive input from users
need to communicate with a polygon controller which has the responsibility to con-
vert user interaction such as mouse click coordinates or keyboard input into polygon
annotation. In this case, neither unidirectional flow through passing props nor the
centralized store using flux architecture can help. We need another mechanism
for these components to communicate with each other. Additionally, this method
should be easy to attach or detach from a module/component or that it can meet
49
Figure 4.6: Horizontal communication problem between modules
the requirements of flexibility and extensibility. Another requirement is that not
only the communication is from one component to one another component but one
event might trigger many different handlers in multiple controllers.
4.4.2 Solution
From all requirements of horizontal communication in the application, we see
that the Observer design pattern perfectly matches our needs. We would define
each event in the application as a subject that all the controllers that are relevant
can subscribe to. When an event is triggered by user interaction or by another
event, the subject will notify all its subscribers about this new event along with
the payload so that every controller function can receive and handle those specific
events.
In our application, we wrapped RxJS, which is a javascript library implementing
Observer pattern for handling events, in a EventCenter class which allows compo-
nents to subscribe to specific subjects or emit a new event to all subscribers of
a subject. RxJS handles the task of broadcasting events and subscriptions while
EventCenter wrapper handles initializing and managing subjects through the sin-
50
Figure 4.7: Event flow in our application
gleton design pattern.
The Figure 4.7 illustrates the event flow in our application. The EventCenter
has the role of managing all the subjects (observable) of the application. All new
subscriptions and new events must pass through this EventCenter.
At first, a new component wanted to subscribe to a subject, before the compo-
nent would mount through the method componentWillMount in the class component
or through the useEffect hook in the function component, the component needs
to get the subject from EventCenter and register a callback function which will be
triggered when the event is fired. The component must also store subscriptions
to unsubscribe from the subscribed subjects before being unmounted to prevent
unhandled exceptions.
1 React.useEffect (() => {
2 const { getSubject } = eventCenter
3 let subscriptions = {
4 [SUBJECTS.MOUSE_CLICK ]: getSubject(SUBJECTS.MOUSE_CLICK)
5 .subscribe ({ next: (e) => handleMouseClick(e) }),
6 }
7
8 return () => {
51
9 Object.keys(subscriptions).forEach(subscription =>
10 subscriptions[subscription ]. unsubscribe ()
11 )
12 }
13 }, [])
Listing 4.4: Events subscription and unsubscription
Every time a component requests a subject to subscribe to, the EventCenter will
check if the corresponding subject exists or not, if not it will create a new subject
of that type. It follows the rule of the singleton pattern for each type of subject;
there will be only one subject instance of that type. This ensures that events of
one subject can be handled correctly by exactly that subject and forward fully to
all subscribed components.
1 import { Subject } from ’rxjs’;
2
3 getSubject = (subjectName) => {
4 if (!( subjectName in this.subjects)) {
5 let newSubject = new Subject ()
6 this.subjects[subjectName] = newSubject
7 }
8 return this.subjects[subjectName]
9 }
Listing 4.5: getSubject() method of EventCenter class
When a component wants to emit an event, it will pass this event along with the
payload to all subscribers of that subject through the emitEvent() method of the
EventCenter class. This method simply gets the corresponding subject of the event
and executes the pass() method of the subject which is implemented by RxJS. All
the subscribers will receive the event and run the registered callback functions.
1 emitEvent = (action) => (data) => {
2 const subject = this.getSubject(action)
3 subject.next(data)
4 }
Listing 4.6: emitEvent() method of EventCenter class
52
Chapter 5
Backend Architecture
In this chapter, we present design of our server implemented using Flask
framework. We describe the usage of cloud storage with browser for han-
dling large data, and representing our the scalable deployment solution for
backend server and deep learning model serving.
5.1 Flask framework
5.1.1 Overview of Flask framework
Flask is a minimal micro web application framework written in Python. Flask
was created in 2004 and has become popular among Python enthusiasts. As of July
2021, it has the second most stars on GitHub among Python web-development
frameworks which is 56.1k, only slightly behind Django. It was also voted the
most popular python web framework in the Python Developers Survey 2020 [28]
as in Figure 5.1. It has various applications in commercial, private, and research
projects. This popularity ensures it has a vibrant ecosystem of extensions, docu-
ments, and tutorials.
Flask is classified as a micro framework because Flask has a simple core but
extensible instead of requiring additional particular tools or libraries by default.
However, Flask supports extensions that can add application features as needed.
Numerous extensions provide database integration, form validation, upload han-
dling, ...
53
Figure 5.1: Most popular python web frameworks in Python Developers Survey 2020
Source: https://www.jetbrains.com/lp/python-developers-survey-2020/
5.1.2 Flask views and routes
In Flask, basically, we use views and routes to route and handle incoming re-
quests:
•
Route: Route is the URL which clients will use to connect to the server. It’s a
way to locate a resource (a resource being some functionality or information).
Flask provides the route decorator to define the matching URL and rules such
as accepting incoming methods for that specific endpoint.
•
View: View can be any callable (like function) that receives requests and
returns the response for that request. Flask is responsible for sending the
response back to the client.
To create a Flask application, we start by creating a Flask object, and then
associate routes to views. Flask takes care of dispatching incoming requests based
on the request URL and the routes you have defined.
1 # app . py 2 from f l a s k import Flask
54
3 4 app = Flask (__name__) 5
6 @app . route ( ’ / h e l l o ’ ) 7 de f index ( ) : 8 re turn "He l lo World ! "
Listing 5.1: A basic Flask application
Then we use command flask run to start the app. It will create a Flask applica-
tion which you can use a web browser to go to http://localhost:5000/hello to
receive the response message “Hello World!”.
The Flask route decorator can be used to indicate and unpack URL parameters.
For example, we can define a new route to extract a name parameter from the
URL to say hello.
1 @app . route ( ’ / h e l l o /<name> ’ ) 2 de f h e l l o (name) : 3 re turn f "He l lo {name} ! "
Listing 5.2: Flask route with parameters
With this new route, we can call the route /hello/Monday, this will return the
message “Hello Monday”.
Similarly, you can define other routes for your application. However, when the
application grows with an increasing number of routes and views, the file app.py
will grow bigger and harder to maintain due to its size and complexity. To solve
this problem, we will use the Flask Blueprint to structure our application so that
it can become clearer to understand and more manageable.
5.1.3 Flask Blueprint
Flask Blueprint is used to compartmentalize your API and give the ability to
structure the application in a more understandable way. For example, in our
annotation application, we need multiple key functions to support different types
of data and requests. More specifically, there must be routes and views to handle
projects, datasets, image and video data, annotations, etc. We can still create
individual routes in app.py as in the left diagram of Figure 5.2
55
Figure 5.2: Example of endpoints in a annotation platform API, where each color indicating grouped functions forming an individual Blueprint and injected into the Flask app
However, there’s clearly some well-defined subsets of functionality in this API.
So we can capture these subsets as individual modules, each defining a Blueprint
for that subset as as in the right diagram of Figure 5.2. For example, we have all
the endpoints related to querying and managing dataset into one Blueprint so that
we can easily handle these functions as a whole and not interfering other functions
belong to different Blueprints.
Each Blueprint is an object that is very similar to a Flask application. They
both can have resources, static files, and views that are associated with routes.
However, a Blueprint needs to be registered in an application to run. In other
ways, you are extending the application with functionalities defined in registered
Blueprints.
Defining a Blueprint is also very similar to defining an application. First, we
create an object of the Blueprint class provided by Flask. Blueprint class comes
with optional arguments for configuration, one commonly used is url_prefix which
adds a prefix for all routes defined in this blueprint.
1 from f l a s k import Bluepr int 2
3 pro j e c t s_b luep r in t = Bluepr int ( ’ p r o j e c t s ’ , __name__, ur l_pre f i x="/ p r o j e c t s " )
56
Listing 5.3: Creating a new Blueprint object for projects
After creating the blueprint object, we can associate routes and views to this
blueprint as we did with the Flask app object.
1 @projects_bluepr int . route ( "/ p r o j e c t_ l i s t " , methods=["GET" ] ) 2 de f get ( ) : 3 re turn j s o n i f y ( p r o j e c t_ l i s t )
Listing 5.4: Associating routes and views to a blueprint
Here we define the get project list endpoint of method GET. The route URL
combined with the defined url_prefix of projects_blueprint is the URL for accessing
this function which is /projects/project_list. Finally, we need to import this
blueprint with the application object in app.py file.
1 app . r e g i s t e r_b lu ep r i n t ( p ro j e c t s_b luep r in t )
Listing 5.5: Registering a blueprint to Flask app
You can define multiple blueprints in one application that each group a subset
of related functionalities and obtain a more concrete project structure.
5.1.4 Request parsing
Additional aspect that we have to handle is parsing and validating HTTP request
objects. Each request may contain data which may be attached to different parts
of the request such as body, header, URL parameters, . . . we need to acquire from.
Additionally, the data from the request may not always be safe and reliable, and
ill-formed data may cause errors for the whole system. In this project, we use the
library webargs to handle these requirements. It supports decorators for routes to
handle parsing and validating with built-in or custom-defined validate functions.
1 from webargs import f i e l d s , v a l i d a t e 2 from webargs . f l a s k p a r s e r import use_args 3
4 @app . route ( "/ h e l l o " ) 5 @use_args ({ 6 "name" : f i e l d s . Str ( r equ i r ed=True , v a l i d a t e=va l i d a t e . Length (min
=3) )
57
7 } , 8 l o c a t i o n="query" 9 )
10 de f h e l l o ( args ) : 11 re turn "He l lo " + args [ "name" ] + " ! "
Listing 5.6: Parsing and validating request using useargs decorator
In the above code snippet, we wrap the predefined hello route with use_args
decorator of webargs. At the specified location is “query” the argument “name” will
be parsed from the request URL. By defining the required option, fields data type
and a length validate function, we required that a string name must be included
in the URL and has minimum length of 3 characters.
5.2 Cloud storage
5.2.1 Problem
Being a data annotation platform, our application needs to have the capability
to store and serve a high load of data, specifically images and videos. Images and
videos are more difficult to handle due to its larger size compared to text data,
requiring more storage to save all of them. Additionally, it also demands higher
server requirements such as throughput or ability to serve heavy files concurrently.
For better users’ experience, those images and videos should also be accessible with
low latency in different regions.
For those requirements, it creates hard constraints for the engine hosting the
application server such as high disk storage, throughput, compute power, . . . . with
additional careful caching strategies to reduce latency. As a result, it creates higher
costs to maintain the hosting machine and also comes with lots of engineering and
configuring jobs.
5.2.2 Solution
For those reasons, we decided to delegate the task of storing and serving to a sep-
arate cloud storage service which is Google Cloud Storage. Cloud Storage provides
buckets with large storage depending on application’s with default configuration
that solves listed problems:
58
•
The app’s load will be reduced by offloading serving static assets to Cloud
Storage. This cut down the high requirements of hosting machine configura-
tion and reduced the cost as well.
•
Google Cloud Storage works as a content delivery network by default itself
without any special configuration. Therefore, the stored data can be accessible
with low latency in multi-region on the world through caching and distributing
into servers across the world.
With separate cloud storage, the flow of serving static content changed as fol-
lows. The backend now stands in the middle of the client and the cloud storage.
Instead of storing and serving files directly, the server will upload files from the
client to the cloud and only store the URLs. Then when the client needs a file,
it requests the server and the server will respond with the URL of the file. The
client can then use this URL to get the file directly from the cloud storage. The
cloud storage is configured for public read access, but limits write access to the
server only. Therefore, we can control which files are allowed to upload to the
storage while allowing ease of access for users. The flow of uploading and getting
files are illustrated in the Figure 5.3. When uploading a file, instead of saving it to
the storage in backend server, the server will upload the file to the cloud storage
bucket then receive and save a URL referencing to that file. Whenever user need
to access a file such as viewing an image when annotating, user will retrieve the
file URL from the server then use that URL to get the corresponding file from the
cloud storage.
5.2.3 Browser caching
With many operations that require loading multiple media files that vary in sizes
from browser. The direct approach that downloading these files again and again
every time clients need to access is very time-consuming and takes throughput to
handle serving these requests. Therefore, we take advantage of browser caching
to minimize number of requests and serving resource. In our application, we ap-
ply two different caching strategies to reduce the data transmission amount from
cloud to user’s devices so as to reduce time waiting for clients while keeping the
client updated with new data and information. We divide our media files into two
categories: rarely update data and frequently update data.
59
Figure 5.3: Upload and get file from Google Cloud Storage flow
Browser caching with max-age
The rarely update data category includes videos and images of dataset. These
dataset files are immutable during the annotation process. There are only two
operations allowed on this kind is uploading new data or delete existed one. This
allow us to cache these data for a long period of time without caring about new
updates as long as they do not take up too much device’s memory. We applied
cache period by setting a cache control header max-age of 604800 seconds which
is equivalent to one week for this type of file. It means that the clients only needs
to load data at the first time they access the files. Later access during cache valid
time, these files will be served right from browser cache without fetching again
from the internet (Figure 5.4). Only after the caching time expired, the browser
requests the file again and renew the caching remaining time for the new file.
Browser caching no-cache
For the frequently update media files, which includes annotation masks that are
also image files created and changed very often due to users annotation process.
We cannot apply a very long cache time for this type of file because it may cause
60
Figure 5.4: Caching by setting cache control max-age
users confusing by displaying an old version of annotation masks which they edited
before. However, they should also be cached to reduce amount of data load as much
as possible, especially on mobile devices. Therefore, we apply a caching control
strategy no-cache. The word "no cache" does not mean the browser won’t cache
anything and reload the resource again all the time when users access. Though,
the browser still keep the file in cache but before using the resource, it will send a
request to check if the current version of the resource in the browser is still up-to-
date (Figure 5.5). If the cloud response that the cached resource is still valid and
usable, the browser will serve the resource from cache. By contrast, it the cloud
response that the file is outdated and need to be updated, browser will fetch the
new file to deliver to users and also update the file in the browser cache. This
strategy can reduce transferring data while ensuring data is always served by its
latest version.
The key difference in two caching strategies is no-cache contents always require
one request to ensure the freshness of the data before using in all the cases. While
max-age strategy only send request to server whenever the cached file’s date is
expired.
61
Figure 5.5: Validating the resource version before using in no-cache strategy
5.3 Deployment
5.3.1 Docker
After developing our application, we need to deploy the server to a production
environment so that the services are accessible for users without starting the server
on their local machines by themselves. For the ease of packaging and deploying, we
use Docker technology which supports dependency to host system and consistent
delivery.
There are two main advantages of using Docker container:
•
Container consistent and isolated environment: Docker provides the
ability to package our application with all its requirements and dependencies
in a Docker container. Containers can be shared and portable to different
machines such as developer local machines, host machines, ... independent to
underlying operating system. Docker containers ensure a consistent behavior
across environments with simple installation.
•
Responsive deployment and scaling: Application packaged into a Docker
container can be deployed into a hosting service with little additional config-
62
uration and auto-scaling ability. Docker containers can apply to dynamically
manage workloads, scaling the application up or down depending on incoming
users request at high availability and corresponding budget. A container can
be replicated into multiple running instances to server the application with
higher load for concurrent incoming requests.
5.3.2 Google Cloud Run
For those advantages and simple configuration of Docker explained in Section
5.3.1, we containerized our application server and hosting on Cloud Run service
provided by Google Cloud. Cloud Run service provides us a ready environment for
managing containers, versioning, and quickly turning an container into a service
that accept requests from the internet. Additionally, it support high flexibility
for automatically scaling the service by replicating more service instances of our
application container or turning off some when the load is low. Incoming requests
are route into one of available container instances to handle.
5.4 Model serving
5.4.1 Problem
Since the manual annotation process is very tedious and might be very slow
for a large dataset. Therefore, we integrate intelligent assistance to our system
as aid to manual drawing process by reducing user’s interactions while improving
performance and quality of annotation results. This is particularly useful when
applying to complex annotation type such as mask annotation and for large work-
load in video annotating. For these aid tools for the annotation process, we take
advantages of deep learning models which have proven their power in these tasks.
As using deep learning models to run prediction for supporting user with their an-
notation tasks, this comes up with the demand of deploying these models so that
these models are available in our service. We specified the requirements that our
models serving solution must fulfill to be applicable in a annotation platform:
•
Real time interference: As being tools support for user annotation process,
our models’ input is dependent on user interactions while they are annotating.
63
Therefore, we cannot do offline prediction to prepare the results in advance.
Additionally, while annotating, users continuously interact with the platform
and receive models’ predictions in their process. For those reasons, the inter-
ference time must be short (count in seconds) so that it does not make users
wait too long for each of their interaction.
•
Allow concurrent requests: As there are not only one deep learning mod-
els are applied in our platform to support the clients but there are multiple
different models that each is for solving one problem. We cannot reload each
model into memory when there are the need of using that model because it
takes significant amount of time. As a result, we cannot run predictions for
multiple models at the same time and limit the usage of tools on client side.
However, the models should be able to run predictions for different models at
the same time for corresponding different tools available for client and also
requests from multiple clients and devices at the same time.
•
Easy deployment: We have multiple models that we inherited and integrated
to platform. Each model is implemented in a different framework and has its
own dependency libraries. Especially, installing deep learning frameworks on
different devices is quite tough and requiring much time. Therefore, the model
serving solution should be able to be deployed onto the hosting machine so
that it meets all models’ dependencies and ensures the right environment for
running each model.
•
Scalable: As there might be many annotators working on the same annota-
tion project and with increasing number of projects and dataset, one machine
managing all the models and running predictions might not have sufficient
computing resource to satisfy the demand. Therefore, model serving solutions
should also be scalable. Meaning that depending on current number requests
and usage from clients, the server can monitor the requiring resources and
expand by adding more instances that have same functionalities and dividing
the workload among instances to ensure availability of the model functions for
the clients.
•
Cost efficient: The deep learning models requires high computational power
to run on, specifically GPU(s) to obtain a fast inference time. That comes
64
along with high cost for the machines. The cost becomes a big problem for
maintaining and scaling up the platform for multiple users. Therefore, we
must save the resource by sharing GPU(s) among different models while only
keeping running instances to minimize the charging fees as much as possible.
5.4.2 Solution
To meet those requirements listed in 5.4.1 sections, we take advantage of model
as service deployment using model serving libraries with optimizations in request
size and caching. Due to hardware limitation which is GPU quota on cloud services,
we cannot apply scaling and cost measuring. However, our current deployment
option can be extensible to scale up using Kubernetes on popular cloud services
such as Google Cloud, AWS. Currently, we deploy our model service on Google
Colab Pro with powerful compute engine at a cheap price while having sufficient
both CPU(s) and GPU(s) for running our deep learning models.
Model as service deployment
There are different approaches for deploying deep learning model such as: edge
deployment, model in service, offline batch prediction, or model as service. How-
ever, each option has its own advantages and disadvantages.
Edge deployment meaning that we first load the models to client’s devices, then
run predictions right on user devices such as computer, laptop, or tablet. How
ever, this solution have many limitation due to subjecting users’ devices which
may have vary in configurations and hardware resources. Additionally, loading
model pre-trained weights onto devices take a lot of storage and very slow due to
large size of models while not providing enough features, also adding difficulty in
monitoring performance and updating the model versions.
Model in service is a deployment that we include our models into backend server
which trigger model prediction as function with input from requests from client.
This does not dependent on client devices, but it creates a strict correlation with
backend hardware and configuration. That is very inflexible because backend and
models have disparate resources demand and deploy a hosting machine that opti-
mized for both backend and models environment. Additionally, this also coupled
the development and scaling process of both services while they are separately
65
Figure 5.6: Model-as-service prediction request and response flow
Source: Deployment and Monitoring
developed process and have different need of scaling levels.
Offline batch prediction may have low latency because it run periodically on
data from database and store back the result to directly serve for users on requests.
However, it is not applicable for real time inference and not running on dynamically
change data.
Model as service is a different approach which decouple the models separately
into a isolation inference service (Figure 5.6). The client and server can interact
with the model by making requests to the model service and receiving responses.
The model as service does not depend on neither client nor server configuration,
which make it is easier to deploy and manage. It is also flexible to scale inde-
pendently with optimal hardware and environment for models to run on. The
drawback of this approach is that it will add latency to the predictions created by
data transmission between client to server and from server to the model service.
Considering all the advantages and disadvantages of these approach, we see that
the last option model-as-service is the most suitable solution for our platform which
directly tackle the hosting machine hardware obstacle while providing acceptable
service on our requirements for model serving.
66
Model serving library
So, running models as a separate service, we need to manage the models which
including loading models into memory, handling incoming requests to return re-
sponses base on model predictions, and sharing resources between models. There
comes model serving libraries that solve all those constrains which are TorchServe
and Tensorflow Serving which is aim to create a fully managed model service.
To run models in a model serving library, first we need to package all the model
dependency which includes model implementation code, utilities functions that
are applied in running the model, the trained model weight, and also list of li-
brary requirements for executing the model. Additionally, we need an additional
customized handler, which will define the prediction process. in the customized
handler, we give instructions for the library to load the model, pre-processing re-
quest before feeding into the model, how to run inference, and parsing the model
output to the response to return for the client. After preparing all the stuffs and
archive them all, we have a portable model package that can be stored at any
place on local machine or cloud storage that can be easily loaded by the library
and provide ready-to-use inference service in one command. The library will han-
dle installing all dependent libraries, loading the models into memory, and open
REST endpoints that accepting incoming requests for prediction.
The model serving libraries handle the most difficult problem for us which is
resource sharing between models. As having multiple models for multiple functions
in our platform, we must have all the models ready to run and also utilizing the
computing power of the GPU for multiple models running at once. It can also
scale the models onto multiple GPUs on the machine to satisfy increasing incoming
requests as many users using the platform at the same time.
This solution can be upgraded to be integrated into a Kubernetes cluster to be
add the ability to scale horizontally to serve a large number of users and efficiently
manage serve machines usage cost.
Optimization
The biggest challenge while deploying model as service is the latency created by
data transmission from client which is used as input for models to run predictions.
Specifically, in our platform, models will receive two types of data, which are images
67
or videos from the dataset and user interactions such as scribbles or text expression.
So as to reduce the latency of calling models service, we try to minimized the
transmission data as much as possible. In this kind of tasks, user interactions are
continuously generated and dynamically changing so that it is forced to send these
information per request. By contrast, dataset data is persisted and unchanged
during its lifetime. Therefore, instead of sending the image or video data from
client to the model service, we only send the data URL to specify the model service
where to obtain the data itself. For each requests, model service will automatically
load data from the cloud storage by given URL and run predictions. This simple
change does not only reduce transmission size so as to reduce to latency, it also
utilize the the strong internet connection and collocation between Google services
resulted in faster loading time without relying on user internet speed. Additionally,
we also applied LRU caching for these image and video to further lower number
of data getting requests made by the service, which is most efficient when a user
have to make multiple annotations on single data instance.
68
Chapter 6
Smart Assistance for
Instance Segmentation with
Referring Expression
In this chapter, we first review the Cross-modal Progressive Comprehen-
sion module and some of our modifications based on our experiments,
then adopt a decoder branch from DeeplabV3+ [29] to improve segmen-
tation performance, which is inspired from [30]. The whole pipeline is in
Figure In terms of video referring expressions segmentation, we proposed
a strategy to fuse masks throughout the video reusing some modules of
MiVOS [31]
6.1 Cross-modal Progressive Comprehension
The original model can separate into two stages: 1) Cross-modal comprehension
at each level separately, 2) Text-guided Features Exchanged and predict mask.
In stage 1, the model builds the Entity Perception module and Relation-aware
Reasoning module to fuse information between text and image then find the rela-
tion between spatial regions in the image as in Figure 6.2. In stage 2, the model
exchanges information between levels then predicts the mask.
69
E ncoder
Stage 1
Text-guided Feature
Exchanged
ConvLSTM
Stage 1
Stage 1
ConvLSTM
ConvLSTM
Conv 1x1
LS TM
Figure 6.1: CMPC Pipeline
6.1.1 Entity Perception
This module aims to produce multi-modal features which contain information
of both language and visual. In particular, it fuses features of words related to ap-
pearance and entity with visual features to find all referred objects in the sentence.
Language encoder
The original model uses GloVE to represent the words and feeds it into an LSTM
to extract features. The output of LSTM is a set L = {l1, l2, ..., lt} where li is feature of i− th word in sentence. Then, it uses two fully connected layer to get 4-D vector
p = {pentity, pattribute, prelation, punnecessary} corresponding with the probability of each
word belong to 4 classes entity, attribute, relation, unnecessary. The whole process
can be formulated as:
pi = softmax(W2σ(W1li + b1) + b2)
where W2 ∈ R4×Cl ,W1 ∈ RCl×Cn , b1, b2 are learnable parameters; σ(.) is sigmoid
function.
70
After predicting the probability of each word, the model creates global linguistic
context of entities for multi-modal fusion by:
qfusion = ∑
li(p entity i + pattributei )
Visual encoder
The model uses DeeplabV3 as image encoder, and get the features extracted
at layer res3, res4, res5 as multi-level features V . Moreover, it uses 8-D spatial
coordination features O to embed more information about location into visual
features such as prior works [32], [33]. In particular, it maps the normal coordinates
into the relative coordinates, where the top-left and bottom-right of the feature map
are (−1,−1) and (+1,+1), respectively. O can be calculated by:
O =
[ 2x
W − 1,
2y
H − 1,
2(x+ 1)
W − 1,
2(y + 1)
H − 1,
2x+ 1
W , 2y + 1
W , 1
W , 1
H
]
Then, the model concatenates O and V at each level before using 1× 1 convolution
layer to generate the visual feature map X containing spatial information.
MUTAN Fusion
After having image features and language features, the model use MUTAN [19]
for generating multi-modal features M at each image feature level. In particular,
the model mapping two features into same dimensions then using element-wise dot
product of both features. It can be formulated by:
q̂i = tanh(qfusionW3i)
X̂i = tanh(XW4i)
Mi = X̂i ⊙ q̂i
M =
r∑ i
Mi
where W3i ∈ RCl×Cn, W4i ∈ RCv×Cn are learnable parameters. r is hyperparame-
ter following tensor sparsity introduced in [19]
71
MUTAN Fusion
Linear
Linear
Linear
Adjacency Matrix
Entity Perception Relation-aware Reasoning
Figure 6.2: CMPC Stage 1
6.1.2 Relation-aware Reasoning
In this step, the original model uses relational words to highlight the referred
object and suppress non-referred one by using fully connected graph. In particular,
the graph is defined as G(V,E,M,A), where V and E are set of vertexes and edges,
M is set of vertex features, A is the adjacency matrix.
The graph consists N = H×W vertexes where H and W are width and height of
multi-modal features M . To construct adjacency matrix, the model first calculates
features for each relational word by:
ri = lip relation i
R = {r1, r2, ..., rT}
Then, the model generates the affinity matrix between M and R by using cross-
modal product attention mechanisms as follow:
B = (MW5)(RW6) ⊤
where B ∈ RN×T measures the relevance between each spatial region onM and each
word of R. The affinity matrix then is normalized by using softmax on dimension
T and dimension N to produce B1 and B2, respectively. Finally, adjacency matrix
72
A is calculated by:
A = B1B ⊤ 2
So, if a region is highly related to a word and that word is highly related to another
region, then two regions are related to each other in a relational context. Moreover,
the normalized function also helps to limit the total contribution to one node to 1.
After constructing the graph G, the model applies graph convolution [34] as:
M̂ = (A+ I)MW7
where W7 ∈ RCm×Cm is learnable parameter, and I is identity matrix.
The graph features M̂ then concatenate with multi-modal features M , spatial
coordinate features O and sentence features s, which is calculated by:
s = ∑ i
li(p entity i + pattributei + prelationi )
After concatenating, the result is fused together by passing through the 1 × 1
convolution layer to get the final features Y for each level.
Our modification
We realized that when applying softmax on dimension T of B to generate B1, if T
is small, the value of B1 no longer represents for the degree of relationship between
that a spatial region with a word. Therefore, we padding zero values to keep T
equal to a hyperparameter k before applying softmax, then remove the padding
values to produce B1. In this thesis, we choose k = 20, which is the maximum
length of the sentence.
6.1.3 Text-Guide Features Exchanged
The authors of original paper proposed Text-Guide Features Exchanged module
to combine features of different levels based on sentence features s. The process
consists multiple rounds which output at round k is Y k i for each level i ∈ {3, 4, 5}.
First, they extract global context vector gi from Yi for i ∈ {3, 4, 5} by using
73
MatMul
MatMul
MatMul
Figure 6.3: Text-guided Feature Exchanged at one round for level 4
affinity matrix between sentence features s and final features Yi as follow:
Ai = (sW8)(YiW9) ⊤
gi = AiYi
Then, the global context vector gi then fuse with sentence features s by using fully
connected layer to generate multi-modal context vector ci. Finally, the multi-modal
vector c is used to weighted features from others level in channel-level, which can
be formulated by:
Y k i =
Y k−1 i +
∑ j∈{3,4,5}\{i} σ(ciW10)⊙ Y k−1
j , k ≥ 1
Yi, k = 0
where W10 ∈ RCm×Cm is learnable parameter. The process of exchange feature
at one round for one level is illustrated in Figure 6.1.3
74
Conv 1x1
E ncoder
CMPC
a green chamelon on the bottom left
Conv 3x3 Rate 6
Conv 3x3 Rate 12
Conv 3x3 Rate 18
Average pooling
C onv 1x1
U psam
pling
C onv 1x1
C onv 3x3
C onv 3x3
C onv 1x1
Figure 6.4: Pipeline after adopt decoder of DeepLabV3+
6.2 Decoder
To enhance the segmentation, we adopt the decoder of DeepLabV3+ [29] into the
model. In particular, we first remove the last 1×1 convolution layer of the original
model, then add Atrous Spatial Pyramid Pooling in [29] to generate features for the
decoder. Finally the features feed-forward 1× 1 convolution layer and upsampling
to the size of low-level features.
In terms of low-level features, we choose the output of res2 then pass through
1 × 1 convolution layer to reduce the dimension. The high-level features and low-
level features are concatenated and decode as in Figure 6.4
6.3 Mask fusion module
In our platform, we also support Video Object Segmentation; therefore, we
proposed a strategy to fuse masks from multiple frames to improve the consistency
throughout the video. In particular, we reuse Fusion Net and Propagation Net of
MiVOS [31] for our algorithms.
The Propagation Net consists of two parts: memory module and prediction
75
module. With the memory module, the Propagation Net store masks as key and
value. Then, the prediction module considers the next frame as a query and uses
the key, value in the memory module to apply attention for predicting mask. The
first frame of the propagation process is called the keyframe. When two keyframes
propagate to the same frame k, the Fusion Net will be used to fuse two propagation
masks by using the distance between k and two nearest keyframes which have
predicted mask of Referring Segmentation system.
To predict mask at frame k, we first add mask of Referring Segmentation system
MR k into memory module of Propagation Net. Then, we use Propagation Net to
propagate fusion mask from the previous frame MF k−α to frame k to produce pk.
We continue applying back-propagation from the mask of future frame MR k+α to
produce fk. Finally, we use Fusion Net to fuse mask fk, pk. So, the information of
the current frame is combined from the future frame and the previous frame and is
guided by the current frame, thereby it enhances the result significantly in terms
of consistency for the Video Object Segmentation task. Our algorithm is described
concretely in Algorithm 1.
Algorithm 1: Mask fusion strategy
Input : MR set of masks predicted by Referring Segmentation system
Output: MF set of masks after fusion
for k ← 1 to n do
Propagation(MR k , k, k) ; // add mask into memory
if k>1 then
pk ← Propagation(MF k−1, k − 1, k) ;
end
if k<n then
fk ← Propagation(MR k+1, k + 1, k) ;
end
MF k = Fusion(pk, k − 1, fk, k + 1) ;
end
6.4 Rule-based algorithm
With the one-stage method, the model is not robust with some attributes, espe-
cially with color attributes. Based on our experiments, the model can not distin-
76
Sentence Analysis
Object Analysis
Instance Segmentation
Matching Modules
Text query
Image Objects
Mask
Referred object attributes
Objects attributes
Final mask
Figure 6.5: Rule-based pipeline
guish some colors such as pink-red and blue-purple. So, we proposed a rule-based
model to handle each specific attribute of the object. Our algorithm is illustrated
in Figure 6.5.
6.4.1 Textual Attribute Extraction
In the current approach, we support the query that follows two rules:
•
The subject of the sentence is the referred object
•
There is one color in the sentence at that color is the main color of the object,
and the color belongs to our color set.
Object detector
To detect the referred object in the sentence, we use the POS tagging module
of Natural Language Toolkit. The module will return a tag for each word in
the sentence. Then, we get the NOUN word which is physical entity based on
hypernyms set of WordNet.
Color detector
To detect color in the sentence, we use a predefined color set of CSS3 web colors,
which contains 147 colors in total. Our color set has simple colors such as red, green,
and blue and has some blend colors such as yellow-green, white smoke, orange-red,
aqua, and navy. After finding the color word in the sentence, we convert it into
RGB color for color matching.
77
Gender detector
From the hypernyms set of the the object detected before, we check is there
any synset of male or female that belongs to human, then set the value for this
attribute. There are three values for this attribute: male, female, unknown.
6.4.2 Visual Attribute Extraction
In terms of visual attribute extraction, we first use DETR panoptic segmentation
to get objects’ masks then keep objects have high confidence scores. Then, we pass
it through a color feature encoder for each object to get the color feature for color
matching.
Color feature encoder
We extract top k dominant color by using Modified Medium Cut Quantization.
The output of the algorithm is k color that represent for all the color in the image.
Gender detector
With each person, we forward its segmentation through facelib to predict gender.
The facelib first detect face of the person then predict whether that person is male
or female. So, our system only support for person having clear face.
6.4.3 Matching module
Object matching
To make the query more flexible, we use WordNet to calculate the similarity
Sobject between the object in the text query and the class of objects predicted in
the image. So, "the person" can match with "the man." However, this matching
does not consider other attributes as gender (man or woman) or age (kid or adult).
78
Color matching
With each object, we find the color in k colors that nearest the color of the
query. The distance between two colors is calculated by:
∆R = R1 −R2
∆B = B1 −B2
∆G = G1 −G2
r̂ = R1 +R2
2
∆C =
√ (2 +
r̂
256 )×∆R2 + 4×∆G2 + (2 +
256− r̂
256 )×∆B2
Then score Scolor = ∆C 756 , where the scale factor is used to normalize the score to
the range (0, 1)
Gender matching
First, we check if there is gender constraint for referred object in query sentence,
then if a person match with the query, the score Sgender = 1, and otherwise.
Retrieve best-matched object
To find a best-matched object, we compare each attribute in the query with
the corresponding attribute in the objects, then weights the scores to produce a
matching score with each object as following:
Stotal j = α1S
color j + α2S
object j α3S
gender j
79
Chapter 7
Smart Annotation Platform
In this chapter, we present the our smart annotation platform which con-
sisting of two main features of an labeling application: data management
and labeling tools for image and video object segmentation. For labeling
tools, we integrated the proposed Referring Expression and Mask Propaga-
tion as aid for the users to speed up the annotation process. The overall
architecture, and features of the application are shown in this chapter.
7.1 Overall architecture
In overall, our annotation platform is a web application composed of five main
components including: front end, back end, model serving server, database and
cloud storage (Figure 7.1). The detail description of each components is described
in Chapter 4 and 5. Each component is hosted on a dedicated server depending
on requirements of each service and communicate with others by REST API via
HTTPs protocol. With separated concerns, each service can be developed and
deployed independently.
Although native application can provide more access control on user devices
which can take advantages of device hardware for greater performance, we decided
to develop our annotation platform as a web application to extend its wide range
of usage not limiting to specific devices and operating systems. Furthermore, an
application can be directly accessed and used instantly by users without additional
installation and complex configuration by end users. With those points, our plat-
form can extend its usage not only to expert users but also untrained ones which
80
Figure 7.1: Overall architecture of our annotation platform
take a high proportion in data annotation workforce. It also can extend the means
of annotating by allowing operation on mobile devices such as tablets with higher
portability and wider range of gestures based on built-in touch or voice input on
those devices.
7.2 Features
7.2.1 Data management
Data management allow users to manage their projects, dataset, and labels for
annotation tasks. Through this, they can structure their annotation work and
handle all the data.
Projects
Projects is the largest unit in our application which can consists of multiple
dataset and labels. Each project represents for a sophisticated purpose such as
creating data for training self-driving cars or animal segmentation. The Figure 7.2
shows the layout of project management page where users can see all of their created
81
projects. Users can create a new project with description or deleting existing
projects.
Figure 7.2: Layout of project management page
A project might contain both image or video dataset with shared labels for
annotating all dataset in the same project. A project is divided into dataset so
that users can easy organize data that can vary in data distribution from different
data sources or develop/val/test splitting in the model training process. Figure 7.3
show the layout of an example project for animal segmentation data preparation
project. We can see datasets are divided base on data source for wild animals
and pets which can be applied to evaluate models’ performance on different data
domain. Each domain is further divided into dev and test set which can is applied
in validating models.
Dataset
A dataset is belong to one project and it is place to manage all the data instances
for a specific purpose or domain. All data instances in one dataset must be in the
same predefined data type (image or video) to guarantee the consistent in output
annotation format and dataset usage.
Users can view dataset information, update, or deleting existing dataset along
with all data instances it includes. They can also upload more images or videos
to the dataset which will be stored on cloud storage for using in annotation pro-
82
Figure 7.3: Layout of dataset management page of one project
cess. The Figure 7.4 shows the layout a dataset information page which contains
paginated list of upload data instances.
Labels
Labels defining the classes of object in a project (such as cat, dog, bear, etc.
in animal data project), which is shared among all dataset belongs to the same
project. Users can create multiple labels with unique names to represent for the
class, each can have different color for visualizing purpose of annotations in labeling
tools. Figure 7.5 shows the view of creating a new label for the project with list of
created labels in the background. Updating or deleting a label will affect all the
created annotations of all dataset in that project.
7.2.2 Labeling tools
Overview
Figure 7.6 shows the main layout of annotation page. On our platform, video
and image data type run on the same page with the same layout. However, the
video will have three additional components which are video controller, annotation
propagation control, and frame list. Layout and functions for images are keep
the same so that users can easily switch between any data type without getting
83
Figure 7.4: Layout of dataset info page which showing dataset detail and data instances belong to that dataset
to familiar with the user interface again. In this section, to fully describe all
functionalities in our platform, we will give description in respecting to video and
images will behave the same as one frame in a video.
There are five main regions on the layout. At the central, there is the canvas
which contain image or current frame of the video with overlay of annotations
drawn by users. For changing the view port, user can drag the screen and user
scrolling the zoom in/out or they can touch or pinching on touch devices. On the
left side is the mode controller where users can choose one of tools from the list.
On the top navigation bar, there are buttons to trigger function, text or number
input to modify configurations corresponding to current using mode/tool. The
right sidebar display list of annotation objects, data instances, and labels which
allow users to have an overview and interaction to select or editing existing ones.
The bottom part is only applicable for video data instances that consists of video
controller, propagation controller, and frame listing.
Mode controller
Mode controller consist a list of tools which allow users select a tool to interact
with the canvas or a tool to draw the annotations corresponding to three different
84
Figure 7.5: Layout of label management in a project. Creating a new label will open a pop up for user to fill in name and choose colors.
annotation types: bounding box, polygon, and mask.
There are two tools controlling interaction with the canvas view port which are
cursor and select modes. Cursor mode allow users freely dragging and zooming
the canvas while blocking interaction with drawn annotations. Users can use this
to preview annotations without accidentally trigger labeling functions. The select
mode is similar to cursor mode with only one difference is that user can click over
an annotation on the canvas to select the corresponding annotation object and
the mode controller will automatically switch to appropriate editing mode for the
selected annotation.
The other tools are modes for drawing specific types of annotation. For bounding
box and polygon, we only support manual drawing function without no intelligent
support. For mask format, we have two intelligent modes which are scribbles to
mask and referring expression to mask.
Selecting a mode will display the corresponding tool configurations on the top
navigation bar for that mode. These mode are listed here but not limited. We
can implement and integrate more modes and tools to the platform to extend its
features. For example, we can introduce smart tools with suggestion or enhance-
ment for drawing polygon and more without changing current implementation and
infrastructure.
85
Figure 7.6: Layout of video annotation page
Object list
Object list contain all annotation objects in current video (Figure 7.7). An
object represent for one thing (a person, an animal, an item, etc.) for the full
duration of the video. An object consists of many annotations that each annotation
mark the location of the object in one specific frame. Normally, an object will have
the number of annotations equal to number of frames in the video. However, in
cases for occlusion or the object go outside of the camera, there might be frames
with no annotation indicating the disappearance of the object at that frame.
A new annotation object will be automatically created when user draws a new
annotation without selecting any object. When clicking on an object on the list,
the application will trigger corresponding edit mode for that annotation type and
displaying a select which allow user to choose the label for that object. Selecting
a label will be applied for all annotations belong to that object and change the
displaying color on the canvas. User can also toggle the object visibility to hide all
object’s annotations or delete to remove the object itself and all of its annotations.
86
Figure 7.7: Layout of annotation object list
Data instances list
Data instances list panel will list out all the videos/images belong to annotating
dataset (Figure 7.8). It gives user the ability to quick navigation between differ-
ent data instances for annotating or previewing the annotation results. The data
instances list is divided into pages and has a pagination component allowing user
to go to specific page while saving the initial loading time for large dataset.
Label list
The label list panel contains label options that users can assign to their annota-
tion objects (Figure 7.9). These label classes are consistent for all datasets in one
project. User can edit the label including label name, fill color, and stroke color
to enhance visualization of the displaying canvas with greater separation among
objects.
Video controller
The video controller located at the bottom of the screen and only show up when
the annotating data instance is a video (Figure 7.10). Video controller allow user
87
Figure 7.8: Layout of data instance list panel
to navigating between frames of annotating video. User can play the video by using
the "play/pause" button. There are also buttons that allow users to go forward or
backward one frame or 10 frames, this is useful when users are drawing annotations
for consecutive frames. In the middle is the video track which show the current
progress of playing video. Users can go to a frame quickly by dragging the slider
or clicking on the track. Additionally, on the right side show the playing state in
time and frame indexes over the total time and number of frames in the video.
User can also go to a frame by inputting an exact frame index.
Frame list and propagation controller
Figure 7.11 shows the layout the frame list and propagation controller The
frame list will display as a row of frames for current video. Each frame has a
styling representing the annotating state of that frame. One frame can have one of
these three state: no annotation (blank box), key frame - user drawn annotation
(filled box), or propagated annotation (stripe box). This frame list helps users
controlling the annotation process by knowing which frames need to be annotated
or propagated frames which need to be reviewed to ensure the quality. Annotators
can also quickly navigating to a specific frame by clicking the box of that frame.
Swiping to switch frames is also allowed.
Propagation controller is the controller for propagating annotations between
88
Figure 7.9: Layout of label list panel
Figure 7.10: Layout of video controller
frames. Drawing a new annotation can require a lot of detail interactions and a
lot of time, especially for mask annotations. Additionally, annotations only differ
slightly in relative frames. Therefore, propagation module is applied to save the
annotator’s effort. The propagation flow is described in detail in 7.4.
The propagation controller has two main parts: configuration and trigger but-
ton. The configuration contains a select to decide propagation direction (forward
or backward) and an input for specifying the number of frames to propagate to.
The propagation trigger button will be "backward" or "forward" depending on
selected option. After setting configuration and click the trigger button, propa-
gation process will start and the trigger button will become cancel button which
allow users to cancel the running propagation process. While propagating, frame
list will also show a loader on boxes of propagating frame to show the status and
updating frame’s state accordingly to the propagation result. While propagating,
89
Figure 7.11: Layout of frame list and propagation control
user can choose to go to a propagating frame and view an approximate preview of
propagation result to estimating next frame which need user modification before
continue propagating or early stopping the annotation process if the result go too
badly.
Drawing bounding box
Bounding boxes are one of the most popular annotation technique in computer
vision for object classification and object detection. A bounding box annotation is
a rectangle fit the target object which give information about object localization
in one image/frame. It is simpler and consume less time to create comparing to
other annotation types. Our platform allow users to create bounding boxes for
object by clicking 2 top left and bottom right points of the object, then choose the
correspondent label. Annotator can later edit by dragging the shape or resizing
both rectangle dimensions (Figure 7.12).
Drawing polygon
Polygon annotation is applicable for irregular and complex shape of objects.
It gives more precise description for the objects comparing to a bounding box.
However, drawing polygon requires more effort and clicks following the object’s
edges. In our platform, we not only support creating regular polygon annotations
but we also introduce cutting and multiple polygon annotations. Those features
allow us creating inch-perfect annotations in extreme cases of occlusion or an object
consisting of splitting parts (Figure 7.13a), or objects that have cutting hole inside
(Figure 7.13b).
90
Figure 7.12: Bounding box annotations
(a) Multi-parts object (b) Object with holes
Figure 7.13: Polygon annotations
Drawing mask
Mask adds additional levels of detail to annotations at pixel level comparing to
bounding box and polygon. Therefore, it also adds the complexity to the task of
drawing mask. To helps annotator, we introduce two modules as different methods
for generating mask annotations just from weak annotations which are much sim-
pler and relax for annotator to accomplish. Our integrated methods diverse from
localization information (scribbles) to natural language description (referring ex-
pression) that allow user have different experiences and choices for their annotation
process.
91
Scribbles to mask
Scribbles to mask allow users to produce mask by drawing scribble (line seg-
ments) marking object and non-object location on the canvas through positive
and negative scribbles (Figure 7.14b). Explicitly, annotators will use positive or
negative brush to specifying which zones are belong to the object and which are
not instead of carefully brushing pixels by pixels. They are only required to draw
scribbles. After drawing scribbles, they can run scribbles to mask prediction which
is backed by Scribble-to-Mask module proposed in [35] by Ho Kei Cheng et al.
The server will return a gray scale image with each pixel value is corresponding
to confidence score of being lie within the object. The annotators then can jus-
tify the threshold to finalize the object mask or adding scribbles to fix errors in
predicted mask. These functions are available to the users through tools list on
the top navigation bar with each button matching to one function (Figure 7.14a).
After achieve the final mask, users can choose to decide to keep or clean up drawn
scribbles depending on their want.
Referring expression to mask
We also introduce a new mean of annotation which takes the advantages from the
expressive power of natural language which is referring expression. Distinguishing
from popular weak annotation types which are also in the image domain which
targeting at giving localization information of the goal object, we propose referring
expression as untraditional way through a cross domain language-vision channel.
Unlike coordinates given through scribbles or bounding box, referring expression
present the object through fine-grained description attributes such as category, size,
color, relative position, ... which is more general and applicable. For example, when
an object moving in a video, location information can be changed an not reusable
for multiple video frames. By contrast, the outlook of an object is more likely
to be constant throughout the video and in hence be more stable and consistent.
Additionally, while many AI assisted tools performance highly depend on pre-
trained dataset and the result might drop when applying to unseen classes, the
expression of object’s properties is more flexible when applying for various different
kind of annotation categories.
The use of referring expression in our platform is simply require user typing
92
(a) Scribbles to mask controller
(b) Sample result of scribbles to mask. Positive scribbles are green and negative scribbles are red.
Figure 7.14: Scribbles to mask
in the description for the object and call prediction, server will return the mask
depending on the image and given expression. Depending on resulting mask, user
can refine the expression and re-run the prediction to get more precise mask. This
text to mask method is also extensible to speech to mask by taking advantages of
built-in speech to text on mobile devices. Figure 7.15a shows the layout of referring
expression controller.
7.3 Annotation with intelligent assistance flow
Unlike offline platform which can directly run deep learning model utilizing hard-
ware resources on user devices, annotation platform on web environment require
no installation or system configuration. However, the trade-off for ease of install is
the limitation on resource on client side so most of computation must be done on
93
(a) Referring expression to mask controller which consists of a text input with a button to run prediction.
(b) Sample result of referring expression to mask
Figure 7.15: Referring expression to mask
the server. This come with a consequence is that although our integrated models
can meet real time inference, the data transfer overhead add a significant amount
of time to the total time for one turn of model prediction. Even after reducing
the data package size as much as possible (described in 5.4.2), it cannot return
instant result immediately for each of user interaction. Besides that, continuously
sending data for prediction might be wasteful as it will consume a lot of resource
and network or even user’s money if using mobile data. It also causes blocking and
forces user to wait after every interaction so making up a terrible user experience.
For those reasons, we decide to let user trigger model prediction by themselves
instead of spawning prediction automatically. The annotation flow is illustrated in
Figure 7.16.
At first, users will provides scribbles or a referring expression as initial informa-
tion for first prediction. The number of scribbles or the expression’s length and
detail will depend on the performance of the models and experience users can gain
94
Figure 7.16: Annotation with intelligent assistance flow
95
a perception on how much is enough. Then, users can click prediction button or
using a keyboard shortcut to send their input to server for running prediction.
It would take seconds until client receive the response. The response will be
visualized on canvas for users to choose next action.
If the returned mask is good, they can commit and that annotation will be saved.
Or in case the returned one is bad, user can add one or more scribbles, or editing
the expression which is more specifically in fixing error base on returned prediction
result. For example, the mask returned by scribbles to mask is including one
relative background region, user can add negative scribbles to exclude that region.
After editing, user can trigger prediction again to receive an updated result based
on these new instructions. This iterative loop continues until users feel satisfy with
result annotation.
Applying this annotation flow, we can both save user network and waiting time,
while utilizing deep learning model to speed up annotation process on easy to use
web environment.
7.4 Propagation flow
In this section, we introduce our predefined pipeline which supports to annotate
more efficiency and more robust. In previous annotation platform, we observed
that user needs to wait for propagation result to know where is the next frame
that user should provide a manual annotation to refine the segmentation, how-
ever, propagation without skip frame does not always process in real time. It
raises a problem that user and model work sequentially, which is unnecessary and
time-consuming. Thereby, we proposed a strategy Preview-and-Annotate to ad-
dress the issue by reducing the waiting time, and helps user work concurrent with
propagation model.
Annotation propagation can work well for simple cases such as object moving
slowly or distinctive background but the result will drop as the scene become
more complex. However, it is hard for users to have an estimation about the
propagation result for choosing number of frames to propagate to which is about
knowing nearest frame user must hands on annotating next. Therefore, we put in
place a separate sequence so that users can get a preview of propagation results
quickly to adjust the annotation at the right frame without wasting time waiting
96
Figure 7.17: Preview-and-Annotation diagram
97
for inadequate result. The flow is illustrated in Figure 7.17.
The main process of propagation will run by every batch of 5 frames instead of
running all frames at once. Batching prediction allows users to receive consecutive
result continuously rather than waiting for the whole all frames result which might
takes much more significant time. This also add the ability for users to stop the
propagation process immediately when they see the result become unsatisfactory.
Besides that, we let users go to any frame which is currently in propagation
process to preview a draft propagation result. This draft result is generated by
starting another propagation process parallel with the running one, however with
a skip frame rate so that lessen the computation and can give result in a short
time. The skip frame may reduce the real quality of the resulting annotation
but it is still a good approximation for users to decide continuing the annotation
process at faulty frames. Once the user start editing annotation at one frame (the
frame with bad preview result), that frame will be marked as key frame and the
main propagation process will be halted without overwrite when reaching user-
defined annotations at key frames. Key frames’ annotations will be used as base
for propagating later frames.
Combining usage of batch prediction and skip frame preview allow us to offer
users a interactive experience without blocking by a long running process and
increase productivity with an AI-assisted workflow.
98
Chapter 8
Conclusion
In this chapter, we summarize and evaluate our works in terms of appli-
cation and algorithm efficiency, then, propose the future work to enhance
the user experience besides improving the effectiveness of our algorithms.
8.1 Platform Summary
During the process of the thesis’s research, we build a deep understanding of
video object segmentation tasks in computer vision. We also explore and get knowl-
edge about the importance of data annotation tasks in developing deep learning
models. And then, see the potential of the data labeling industry and the role of
annotation platforms in assisting for the intensive annotation process.
Specifically, we study existing annotation platforms to find out the requirements
of building an annotation platform with a comparison between features and sup-
ported labeling methods on multiple platforms. After that, we select our core
functions and combine the advantages of different platforms as ideas for building
our own.
In the process of developing our annotation platform, we study React and Flask
framework for building front-end and back-end with an enhanced infrastructure
for an effective and flexible annotation website. We also learn about hosting our
website and server on Google cloud platform and get the use of cloud storage and
container Docker to handle big data storing with the ability to easily deploy and
scalable. Besides that, we investigate methods of serving deep learning models in
conditions of portability and utilizing computing resources. To reduce users’ wait-
99
ing time and saving network data, we also conduct experiments trying alternative
methods to reduce transferring data so as to produce predictions in a short time
with a smaller data package.
We released an annotation platform with 2 cores components that are data
management and labeling tools. We implemented 3 basics annotation types for
image and video data which are: bounding box, polygon, and mask. We also
corporate annotation assistant which allowing convert users’ scribbles to mask with
accuracy at pixel-level. For video, we integrated propagation modules and mask
fusion to let users only label a small number of frames instead of frame by frame
to reduce human work with a shorter time.
Later, we introduce the usage of referring expression as a new potential inter-
action of annotation method which is more natural by using language for com-
municating with devices while being more general and having higher expressive
power in describing target annotation object. This referring expression method
can be a starting point for examining other language-based gestures in labeling
tasks to be more user friendly and taking advantage of the expressive strength of
languages such as using speech to annotate, combining with other types to reduces
the number of interactions, finding objects in the project for data management,
etc. Throughout this project, we apply and modify prior work and use referring
expression to annotate objects, which can be used to produce initial masks and
users can refine them.
8.2 Algorithm evaluation
In terms of algorithm efficiency, we conducts ablation study to verify our modifi-
cation help to improve result and reduce weight of model. We evaluate the original
model and our modification on the dataset Youtube-ReferVOS. The dataset con-
tains 3,978 videos and 15k expressions referred, of which the train set has 3,471
videos and the validation set has 202/502 videos.
8.2.1 Implementation Detail
Because our model is referring segmentation on image, we filtered all the frames
that exists referred object for training, which total is 312k training sample. We use
100
pretrained DeeplabV3-Resnet101 as visual backbone, and all the hyper-parameters
are the same as the original model, where k = 5 for MUTAN Fusion; Cm = 500. We
train each configuration for 110k iterations using Adam optimization. Moreover,
the original implementation also uses multi-level loss. Concretely, they feed Yi
forward 1×1 convolution layer to predict masks and use Binary Cross-Entropy loss
for all predicted masks. The total loss is computed by:
Ltotal = 0.7Llast + 0.1Lres3 + 0.1Lres4 + 0.1Lres5
where Llast, Lres3, Lres4, Lres5 are loss of final mask and mask at level 3, 4, 5 respec-
tively.
8.2.2 Evaluation Metrics
We evaluate our methods by using two scores: Region Jaccard (J) and Boundary
F measure (F ). Region Jaccard scores measures the intersection area over union
area, whereas Boundary F measures how good of the object boundary.
8.2.3 Ablation Study
The experimental results are shown in Table 8.1. The original model achieves
0.411 overall J&F score, where J score is 0.415 and F score is 0.407. Based on our
observation, the masks which is predicted at level res3 for computing multi-level
loss, are bad as in Figure 8.1. So, we hypothesize that the information at level res3
will have less semantic information, which does not contribute much to the result,
whereas it costs about one-third number of learnable parameters.
On the second version, we try to remove that branch and it degrades the perfor-
J F J&F
CMPC original 0.415 0.407 0.411 CMPCv2 0.405 0.378 0.392 CMPCv2 + Decoder 0,425 0,431 0.428 CMPCv3 + Decoder 0.439 0.439 0.439 CMPCv3 + Decoder + Mask Fusion 0.467 0.481 0.473
Table 8.1: Ablation study
101
Figure 8.1: Predicted mask at each level for calculating multi-level loss. Mask from left to right corresponds to mask at level res3, res4, res5, and final mask
mance from 0.411 to 0.392 with overall J&F score. However, when looking into J
scores and F scores, we realized that only the performance of mask boundary was
affected significantly, which decreases from 0.407 to 0.378. To address this issue,
we think it’s better to use a decoder that specializes in predicting masks. Our next
modification is adopting the decoder of DeeplabV3+ into the model, which makes
the performance of the model to increase significantly. In particular, our model
achieved 0.428, 0.425, 0.431 respectively for the overall J&F score, J score, and F
score.
On next version, we applied our modification which is mentioned in 6.1.2, which
help model slightly improves from 0.428 to 0.439 with overall J&F score. Finally,
we use the mask fusion algorithm to improve the consistency of the prediction
throughout the video. The result increase from 0.439 to 0.473 with overall J&F .
8.3 Experiment results
In this section, we run test cases about different categories to test the efficiency
and quality on real data. From that, we can see limitations in current models
102
Figure 8.2: The models run well on query with keyword "left" and "right" in case of having less than three objects in the image.
and pointing out applicable use cases of each approach. The test cases are run on
our platform with query input by users and the users are allowed to adjust the
threshold of the mask the achieve the most appropriate result on the query.
8.3.1 CMPC model
Object position
This includes query relating to positions of object as referencing to the whole
image with the ability to separate nearby objects. Those expressions often consist
of common position prepositions: "left", "right", "top", "down", etc.
We observe that the model run very well on query with keyword "left" and
"right" for about two to three objects of same kind in the image (Figure 8.2).
However, in more complex scenes with many people or objects of different
classes, the model cannot predict a good mask while having the odd to merg-
ing multiple nearby objects. It also see difficulty in generating mask for position
keyword different from "left" and "right". The result is illustrated in Figure 8.3.
The model can also run in cases of relative position queries such as: "next to",
103
Figure 8.3: Fail case with many objects in the scene
"near", or description for order in the image (Figure 8.4). But it is very stable
with wrong mask in some cases.
Animal
For this category, we aim to test the model’s ability to recognize animals of
different types, which including images contain multiple instances of one animal,
images of multiple animal kinds, or trying to giving wrong query to examine models
output.
The results show that the model work very well in recognizing different kinds
of animals. Especially in hard scene with multiple animal types, the model return
the appropriate mask result when we change the query to a specify animal. To
emphasize the model’s ability to recognize animal kinds, we tested on simple query
with only animal names without any additional clues about position or colors.
The model also yield good results by returning nearly empty mask or mask with
relatively low confidence for wrong queries. However, the model also fail in complex
case with too many animals in one image. Those results are shown in Figure 8.6.
104
Figure 8.4: Model performance in cases of query described by relative position to other objects
Figure 8.5: Model performance in querying different animal kinds
105
Figure 8.6: Model does not work well in case of complex scene with many animals while it can understand wrong queries.
Color and size
We tried running the model on color interpretation from single color to combined
colors query. The results illustrate that, the model can work nicely with single
color objects with different colors especially: red, green, blue, black, white, yellow
(Figure 8.7).
With queries relating to size of objects, the model tends to work better on
queries relating to "big" or "large" keyword while giving worse result on "small"
ones. However, the mask is not perfect in all the correct cases. Some output results
with contrasting size of objects are shown in Figure 8.8.
Others
We also conducted experiments on other kinds of objects such as furniture, and
instruments. The experiments show that current model hasn’t worked well on these
categories. Additionally, we did an experiment on distinguishing genders of person
as well. The model only work in a very rare case with image from side view of
woman and man. It might be from features about hair length and body shape.
106
Figure 8.7: The model performs pretty well for expressions with colors
Figure 8.8: The model has the sense about size of objects. However the masks are not very good in this kind of query.
107
Figure 8.9: Results of the model with query about genders and some kinds of in-house objects.
Those results can be viewed from Figure 8.9.
Summary
In summary, our current CMPC model can work well on two main type of
queries: positions in the image and animal kinds. It also give a good but not
stable results on relative positions, color, and size. Furniture, instrument, and
gender are categories and properties that our model perform unsatisfactorily.
8.4 Future work
To fulfill the annotation task in computer vision, we need to add supports for
other annotation types such as landmark annotation, skeletal annotation, etc. as
well as experimenting effective and suitable intelligent support for those types.
Another important concert of labeling platform is data security and privacy.
In current version, we have not concentrate on this aspect yet and there must be
appropriate data protection mechanisms as well as control accessing to specific data
by roles. We also want to enhance the data development process by implementing
108
workflow and procedure for annotators to work as a team with task management
and methods for label quality control.
Although we are using Docker to containerize our application for scalability and
portability deployment, our platform still consists of multiple independent mod-
ules (front-end, server, model serving), requiring handling deployment separately
and communication between modules. In the future, we want to apply a container
orchestration tool (Kubernetes) for even more consistency and simplicity in de-
ploying and scaling multiple containers at once. We also want to integrate CI/CD
to mitigate manual development tasks while developing the application.
Annotation System with Referring Expression still has many data management
applications, not just using for prediction. Moreover, the rule-based approach is not
limited by current attributes. It can be extended to support more attributes such as
gender, age, unknown categories, or new categories with few samples. That makes
our system become more flexible and can be personalized for particular annotation
projects. In terms of language structure, we will exploit more information by using
a dependency parsing tree
109
References
[1] Research, C., Data engineering, preparation, and labeling for ai 2019, https:
/ / www . cognilytica . com / 2019 / 03 / 06 / report - data - engineering -
preparation-and-labeling-for-ai-2019, Accessed: 2021-07-21, 2019.
[2] Lee, J., (Paul) Natsev, A., Reade, W., Sukthankar, R., and Toderici, G., “The
2nd youtube-8m large-scale video understanding challenge,” in Proceedings of
the European Conference on Computer Vision (ECCV) Workshops, Sep. 2018.
[3] Malasane, A., "decision framework for data labeling strategy." best-in-class
data labeling platform | playment, 2020. [Online]. Available: https://playment.
io/blog/refine-your-data-labeling-strategy-with-a-realistic-
decision-framework.
[4] Chai, J. Y., Hong, P., and Zhou, M. X., “A probabilistic approach to refer-
ence resolution in multimodal user interfaces,” in Proceedings of the 9th In-
ternational Conference on Intelligent User Interfaces, New York, NY, USA:
Association for Computing Machinery, 2004, pp. 70–77, isbn: 1581138156.
doi: 10.1145/964442.964457. [Online]. Available: https://doi.org/10.
1145/964442.964457.
[5] Qiao, Y., Deng, C., and Wu, Q., “Referring expression comprehension: A
survey of methods and datasets,” CoRR, vol. abs/2007.09554, 2020. arXiv:
2007.09554. [Online]. Available: https://arxiv.org/abs/2007.09554.
[6] Wang, P., Wu, Q., Cao, J., Shen, C., Gao, L., and Hengel, A. van den, Neigh-
bourhood watch: Referring expression comprehension via language-guided graph
attention networks, 2018. arXiv: 1812.04794 [cs.CV].
110
[7] Yang, S., Li, G., and Yu, Y., “Cross-modal relationship inference for ground-
ing referring expressions,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), Jun. 2019.
[8] Yu, L., Lin, Z., Shen, X., et al., “Mattnet: Modular attention network for
referring expression comprehension,” CoRR, vol. abs/1801.08186, 2018. arXiv:
1801.08186. [Online]. Available: http://arxiv.org/abs/1801.08186.
[9] Liu, D., Zhang, H., Wu, F., and Zha, Z.-J., Learning to assemble neural module
tree networks for visual grounding, 2019. arXiv: 1812.03299 [cs.CV].
[10] Yang, S., Li, G., and Yu, Y., Graph-structured referring expression reasoning
in the wild, 2020. arXiv: 2004.08814 [cs.CV].
[11] Anderson, P., He, X., Buehler, C., et al., “Bottom-up and top-down attention
for image captioning and VQA,” CoRR, vol. abs/1707.07998, 2017. arXiv:
1707.07998. [Online]. Available: http://arxiv.org/abs/1707.07998.
[12] Liang, C., Wu, Y., Luo, Y., and Yang, Y., “Clawcranenet: Leveraging object-
level relation for text-based video segmentation,” CoRR, vol. abs/2103.10702,
2021. arXiv: 2103.10702. [Online]. Available: https://arxiv.org/abs/
2103.10702.
[13] Yang, J., Huang, Y., Niu, K., Ma, Z., and Wang, L., “Actor and action mod-
ular network for text-based video segmentation,” CoRR, vol. abs/2011.00786,
2020. arXiv: 2011.00786. [Online]. Available: https://arxiv.org/abs/
2011.00786.
[14] Liu, J., Yuan, Z., andWang, C., Towards good practices for multi-modal fusion
in large-scale video classification, 2018. arXiv: 1809.05848 [cs.CV].
[15] Feng, G., Hu, Z., Zhang, L., and Lu, H., “Encoder fusion network with co-
attention embedding for referring image segmentation,” CoRR, vol. abs/2105.01839,
2021. arXiv: 2105.01839. [Online]. Available: https://arxiv.org/abs/
2105.01839.
[16] Feng, Q., Wei, Y., Cheng, M., and Yang, Y., “Decoupled spatial temporal
graphs for generic visual grounding,” CoRR, vol. abs/2103.10191, 2021. arXiv:
2103.10191. [Online]. Available: https://arxiv.org/abs/2103.10191.
111
[17] Yang, Z., Gong, B., Wang, L., Huang, W., Yu, D., and Luo, J., A fast and
accurate one-stage approach to visual grounding, 2019. arXiv: 1908.06354
[cs.CV].
[18] Fukui, A., Park, D. H., Yang, D., Rohrbach, A., Darrell, T., and Rohrbach,
M., “Multimodal compact bilinear pooling for visual question answering and
visual grounding,” CoRR, vol. abs/1606.01847, 2016. arXiv: 1606.01847.
[Online]. Available: http://arxiv.org/abs/1606.01847.
[19] Ben-younes, H., Cadène, R., Cord, M., and Thome, N., “MUTAN: multimodal
tucker fusion for visual question answering,” CoRR, vol. abs/1705.06676,
2017. arXiv: 1705.06676. [Online]. Available: http://arxiv.org/abs/
1705.06676.
[20] Huang, S., Hui, T., Liu, S., et al., “Referring image segmentation via cross-
modal progressive comprehension,” CoRR, vol. abs/2010.00514, 2020. arXiv:
2010.00514. [Online]. Available: https://arxiv.org/abs/2010.00514.
[21] Brooks, J., Coco annotator, https://github.com/jsbroks/coco-annotator,
2019.
[22] Maninis, K.-K., Caelles, S., Pont-Tuset, J., and Gool, L. V., Deep extreme
cut: From extreme points to object segmentation, 2018. arXiv: 1711.09081
[cs.CV].
[23] Manen, S., Gygli, M., Dai, D., and Gool, L. V., Pathtrack: Fast trajectory
annotation with path supervision, 2017. arXiv: 1703.02437 [cs.CV].
[24] Griffin, B. A. and Corso, J. J., Bubblenets: Learning to select the guidance
frame in video object segmentation by deep sorting frames, 2020. arXiv: 1903.
11779 [cs.CV].
[25] Kuznetsova, A., Talati, A., Luo, Y., Simmons, K., and Ferrari, V., Efficient
video annotation with visual interpolation and frame selection guidance, 2020.
arXiv: 2012.12554 [cs.CV].
[26] Inc, S. E., 2020 developer survey, https://insights.stackoverflow.com/
survey/2020, 2020.
[27] npm, I., React library, https://www.npmjs.com/package/react, 2021.
[28] s.r.o, J., Python developers survey 2020 results, https://www.jetbrains.
com/lp/python-developers-survey-2020/, 2020.
112
[29] Chen, L., Papandreou, G., Schroff, F., and Adam, H., “Rethinking atrous
convolution for semantic image segmentation,” CoRR, vol. abs/1706.05587,
2017. arXiv: 1706.05587. [Online]. Available: http://arxiv.org/abs/1706.
05587.
[30] Jing, Y., Kong, T., Wang, W., Wang, L., Li, L., and Tan, T., “Locate then seg-
ment: A strong pipeline for referring image segmentation,” CoRR, vol. abs/2103.16284,
2021. arXiv: 2103.16284. [Online]. Available: https://arxiv.org/abs/
2103.16284.
[31] Cheng, H. K., Tai, Y., and Tang, C., “Modular interactive video object
segmentation: Interaction-to-mask, propagation and difference-aware fusion,”
CoRR, vol. abs/2103.07941, 2021. arXiv: 2103.07941. [Online]. Available:
https://arxiv.org/abs/2103.07941.
[32] Liu, C., Lin, Z., Shen, X., Yang, J., Lu, X., and Yuille, A. L., “Recurrent multi-
modal interaction for referring image segmentation,” CoRR, vol. abs/1703.07939,
2017. arXiv: 1703.07939. [Online]. Available: http://arxiv.org/abs/1703.
07939.
[33] Ye, L., Rochan, M., Liu, Z., and Wang, Y., “Cross-modal self-attention net-
work for referring image segmentation,” CoRR, vol. abs/1904.04745, 2019.
arXiv: 1904.04745. [Online]. Available: http://arxiv.org/abs/1904.
04745.
[34] Kipf, T. N. and Welling, M., “Semi-supervised classification with graph con-
volutional networks,” CoRR, vol. abs/1609.02907, 2016. arXiv: 1609.02907.
[Online]. Available: http://arxiv.org/abs/1609.02907.
[35] Cheng, H. K., Tai, Y.-W., and Tang, C.-K., “Modular interactive video object
segmentation: Interaction-to-mask, propagation and difference-aware fusion,”
in CVPR, 2021.
113